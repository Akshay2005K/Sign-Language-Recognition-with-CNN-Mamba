{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61gqwhoO-mOj",
        "outputId": "9b776c87-7715-4d36-a8d8-f884c1a50f24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mediapipe opencv-python tqdm numpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgYVI4LJ-x_R",
        "outputId": "3147ac07-ce45-444d-992e-8e0682d2a151"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.12/dist-packages (0.10.21)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.9.23)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.7.1)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.7.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.11.0.86)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.25.8)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.2.1)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe) (2.0.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.12 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (1.16.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.23)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Batch Keypoint Extraction (set START_IDX / END_IDX below) ---\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import mediapipe as mp\n",
        "\n",
        "# ====== CONFIG ======\n",
        "DATASET_ROOT = \"/content/drive/MyDrive/AFML/Dataset/WLASL_Videos\"\n",
        "START_IDX = 301   # <-- change per notebook: 200, 300, 400\n",
        "END_IDX   = 400   # <-- change per notebook: 300, 400, 500\n",
        "# ====================\n",
        "\n",
        "# OpenCV can oversubscribe threads on Colab; keep it tame\n",
        "try:\n",
        "    cv2.setNumThreads(1)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def list_gloss_dirs(root):\n",
        "    items = sorted(os.listdir(root))\n",
        "    dirs = [d for d in items if os.path.isdir(os.path.join(root, d))]\n",
        "    return dirs\n",
        "\n",
        "def extract_keypoints_from_frame(results_pose, results_hands):\n",
        "    keypoints = []\n",
        "\n",
        "    # Pose: 33 landmarks (x,y). Use 0s if missing.\n",
        "    if results_pose and results_pose.pose_landmarks:\n",
        "        for lm in results_pose.pose_landmarks.landmark:\n",
        "            keypoints.extend([lm.x, lm.y])\n",
        "    else:\n",
        "        keypoints.extend([0.0, 0.0] * 33)\n",
        "\n",
        "    # Hands: map to LEFT / RIGHT using handedness labels to keep them stable\n",
        "    left_hand  = [0.0, 0.0] * 21\n",
        "    right_hand = [0.0, 0.0] * 21\n",
        "\n",
        "    if results_hands and results_hands.multi_hand_landmarks:\n",
        "        # Build (label, landmarks) pairs if handedness info exists\n",
        "        labeled = []\n",
        "        if results_hands.multi_handedness and \\\n",
        "           len(results_hands.multi_handedness) == len(results_hands.multi_hand_landmarks):\n",
        "            for hmeta, hland in zip(results_hands.multi_handedness, results_hands.multi_hand_landmarks):\n",
        "                label = hmeta.classification[0].label  # 'Left' or 'Right'\n",
        "                labeled.append((label, hland))\n",
        "        else:\n",
        "            # Fallback to whatever order MediaPipe gives\n",
        "            for hland in results_hands.multi_hand_landmarks:\n",
        "                labeled.append((\"Unknown\", hland))\n",
        "\n",
        "        for label, hand_land in labeled:\n",
        "            pts = []\n",
        "            for lm in hand_land.landmark:\n",
        "                pts.extend([lm.x, lm.y])\n",
        "            if label == \"Left\":\n",
        "                left_hand = pts\n",
        "            elif label == \"Right\":\n",
        "                right_hand = pts\n",
        "            else:\n",
        "                # If unknown and a slot is empty, fill right then left\n",
        "                if right_hand == [0.0, 0.0] * 21:\n",
        "                    right_hand = pts\n",
        "                else:\n",
        "                    left_hand = pts\n",
        "\n",
        "    return keypoints + left_hand + right_hand\n",
        "\n",
        "def process_video(video_path, pose_model, hands_model):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        # BGR -> RGB for MediaPipe\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        pose_res = pose_model.process(frame_rgb)\n",
        "        hand_res = hands_model.process(frame_rgb)\n",
        "        frames.append(extract_keypoints_from_frame(pose_res, hand_res))\n",
        "    cap.release()\n",
        "    return np.array(frames, dtype=np.float32)\n",
        "\n",
        "def main():\n",
        "    all_glosses = list_gloss_dirs(DATASET_ROOT)\n",
        "    total = len(all_glosses)\n",
        "    start = max(0, START_IDX)\n",
        "    end = min(END_IDX, total)\n",
        "\n",
        "    if start >= end:\n",
        "        raise ValueError(f\"Empty range: START_IDX={START_IDX}, END_IDX={END_IDX}, total glosses={total}\")\n",
        "\n",
        "    target_glosses = all_glosses[start:end]\n",
        "    print(f\"Total gloss folders: {total}\")\n",
        "    print(f\"Processing index range [{start}:{end}) ‚Üí {len(target_glosses)} glosses\")\n",
        "\n",
        "    # Create MediaPipe models once per notebook\n",
        "    with mp.solutions.pose.Pose() as pose, mp.solutions.hands.Hands(max_num_hands=2) as hands:\n",
        "        for gloss in tqdm(target_glosses, desc=f\"Processing glosses {start}-{end-1}\"):\n",
        "            gloss_path = os.path.join(DATASET_ROOT, gloss)\n",
        "            print(f\"\\n‚û° Gloss: {gloss}\")\n",
        "\n",
        "            try:\n",
        "                files = os.listdir(gloss_path)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Skipping {gloss} (cannot list dir): {e}\")\n",
        "                continue\n",
        "\n",
        "            mp4s = [f for f in files if f.lower().endswith(\".mp4\")]\n",
        "            for file in mp4s:\n",
        "                video_path = os.path.join(gloss_path, file)\n",
        "                output_path = os.path.splitext(video_path)[0] + \".npy\"\n",
        "\n",
        "                if os.path.exists(output_path):\n",
        "                    print(f\"‚úÖ Exists: {file}\")\n",
        "                    continue\n",
        "\n",
        "                print(f\"üé• Extracting: {file}\")\n",
        "                try:\n",
        "                    frames_np = process_video(video_path, pose, hands)\n",
        "                    # Ensure parent dir exists (should already)\n",
        "                    np.save(output_path, frames_np)\n",
        "                    print(f\"üíæ Saved ‚Üí {output_path}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Failed on {file}: {e}\")\n",
        "\n",
        "    print(\"\\n‚úÖ DONE ‚Äî Batch keypoints extracted\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFR8Max7_DUG",
        "outputId": "67239abb-0905-4e5c-ad9f-4f56d9a6ee51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total gloss folders: 500\n",
            "Processing index range [301:400) ‚Üí 99 glosses\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:   0%|          | 0/99 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚û° Gloss: music\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:   1%|          | 1/99 [00:00<00:25,  3.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: 9815.mp4\n",
            "‚úÖ Exists: MUSIC-1584.mp4\n",
            "‚úÖ Exists: 48609.mp4\n",
            "‚úÖ Exists: music.mp4\n",
            "‚úÖ Exists: SignSchool Music, Sing-N2r3NCId7AE.mp4\n",
            "‚úÖ Exists: 1468717488.5224.mp4\n",
            "‚úÖ Exists: 7756.mp4\n",
            "\n",
            "‚û° Gloss: name\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:   2%|‚ñè         | 2/99 [00:00<00:25,  3.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: name.mp4\n",
            "‚úÖ Exists: 48612.mp4\n",
            "‚úÖ Exists: SignSchool Name-DikA_15hmng.mp4\n",
            "‚úÖ Exists: 1468717645.5170.mp4\n",
            "‚úÖ Exists: 7751.mp4\n",
            "\n",
            "‚û° Gloss: near\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:   3%|‚ñé         | 3/99 [00:00<00:24,  3.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: 1468718072.176.mp4\n",
            "‚úÖ Exists: 48810.mp4\n",
            "‚úÖ Exists: 14713.mp4\n",
            "‚úÖ Exists: near.mp4\n",
            "‚úÖ Exists: SignSchool Near 2-mNYYD8m5cdA.mp4\n",
            "‚úÖ Exists: SignSchool Near-FvpESshurCM.mp4\n",
            "‚úÖ Exists: SignSchool Near-NHzMwjY1u8k.mp4\n",
            "‚úÖ Exists: SignSchool Near-xLl0ptpuPfA.mp4\n",
            "\n",
            "‚û° Gloss: necklace\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:   4%|‚ñç         | 4/99 [00:01<00:26,  3.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: necklace-2.mp4\n",
            "‚úÖ Exists: necklace.mp4\n",
            "‚úÖ Exists: SignSchool Jewelry 2-1QExxwdFd8s.mp4\n",
            "‚úÖ Exists: SignSchool Necklace copy-hE1SHCoZwfw.mp4\n",
            "‚úÖ Exists: SignSchool Necklace-SlA5dLXOizo.mp4\n",
            "‚úÖ Exists: NECKLACE-1579.mp4\n",
            "‚úÖ Exists: 2136.mp4\n",
            "‚úÖ Exists: 93972.mp4\n",
            "\n",
            "‚û° Gloss: need\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing glosses 301-399:   6%|‚ñå         | 6/99 [00:01<00:25,  3.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: 1_jXxjd0lKE.mp4\n",
            "‚úÖ Exists: 4Nh1iFv2BMc.mp4\n",
            "‚úÖ Exists: 9563.mp4\n",
            "‚úÖ Exists: Eo-vEE1gFxM.mp4\n",
            "‚úÖ Exists: kgmz82HWI-I.mp4\n",
            "‚úÖ Exists: n5hGwUxGm0Y.mp4\n",
            "‚úÖ Exists: 63549.mp4\n",
            "‚úÖ Exists: RoY7cjIPNxY.mp4\n",
            "‚úÖ Exists: need.mp4\n",
            "‚úÖ Exists: SignSchool Need-0qij0e7-5Ak.mp4\n",
            "‚úÖ Exists: 1468718141.8252.mp4\n",
            "\n",
            "‚û° Gloss: nephew\n",
            "‚úÖ Exists: 48811.mp4\n",
            "‚úÖ Exists: nephew.mp4\n",
            "‚úÖ Exists: SignSchool Nephew-INBtIyECKJ0.mp4\n",
            "‚úÖ Exists: 1468718373.4133.mp4\n",
            "‚úÖ Exists: 6521.mp4\n",
            "\n",
            "‚û° Gloss: never\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:   7%|‚ñã         | 7/99 [00:01<00:22,  4.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: NEVER-1575.mp4\n",
            "‚úÖ Exists: 48813.mp4\n",
            "‚úÖ Exists: never.mp4\n",
            "‚úÖ Exists: SignSchool Never-cTDaclFdVZs.mp4\n",
            "‚úÖ Exists: 1468718677.1119.mp4\n",
            "‚úÖ Exists: 8676.mp4\n",
            "\n",
            "‚û° Gloss: new\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:   8%|‚ñä         | 8/99 [00:02<00:22,  4.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: new.mp4\n",
            "‚úÖ Exists: 9561.mp4\n",
            "‚úÖ Exists: 48814.mp4\n",
            "‚úÖ Exists: SignSchool New 2-WKrrU8zgbyg.mp4\n",
            "‚úÖ Exists: SignSchool New, Fresh-rnNRrzVVohg.mp4\n",
            "‚úÖ Exists: SignSchool New-4xM6zIJhnJ0.mp4\n",
            "‚úÖ Exists: SignSchool New-SgAqaWGs-2o.mp4\n",
            "‚úÖ Exists: 1468718701.4785.mp4\n",
            "\n",
            "‚û° Gloss: newspaper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:   9%|‚ñâ         | 9/99 [00:02<00:22,  3.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: 48815.mp4\n",
            "‚úÖ Exists: 1468718854.2103.mp4\n",
            "‚úÖ Exists: 8898.mp4\n",
            "\n",
            "‚û° Gloss: nice\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  10%|‚ñà         | 10/99 [00:02<00:21,  4.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: 111066.mp4\n",
            "‚úÖ Exists: nice.mp4\n",
            "‚úÖ Exists: SignSchool Nice-WHPAZyGhilQ.mp4\n",
            "‚úÖ Exists: 1468718991.3483.mp4\n",
            "‚úÖ Exists: 7443.mp4\n",
            "\n",
            "‚û° Gloss: niece\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  11%|‚ñà         | 11/99 [00:02<00:20,  4.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: NIECE-1571.mp4\n",
            "‚úÖ Exists: 48820.mp4\n",
            "‚úÖ Exists: neice.mp4\n",
            "‚úÖ Exists: SignSchool Niece-9toTZ-aMVWs.mp4\n",
            "‚úÖ Exists: 1468719070.6193.mp4\n",
            "‚úÖ Exists: 5890.mp4\n",
            "\n",
            "‚û° Gloss: night\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  12%|‚ñà‚ñè        | 12/99 [00:03<00:22,  3.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: 48821.mp4\n",
            "‚úÖ Exists: SignSchool Night-sXPC0bQ0y2g.mp4\n",
            "‚úÖ Exists: 1468719112.7545.mp4\n",
            "‚úÖ Exists: 7109.mp4\n",
            "\n",
            "‚û° Gloss: no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  13%|‚ñà‚ñé        | 13/99 [00:03<00:21,  3.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: 1_jXxjd0lKE.mp4\n",
            "‚úÖ Exists: no.mp4\n",
            "‚úÖ Exists: BqmquW0f1ok.mp4\n",
            "‚úÖ Exists: nyUDOPmQlZQ.mp4\n",
            "‚úÖ Exists: pkzfT9cYvH0.mp4\n",
            "‚úÖ Exists: QJXKaOSyl4o.mp4\n",
            "‚úÖ Exists: SignSchool No-hwa_U71nr74.mp4\n",
            "‚úÖ Exists: 1468719269.5533.mp4\n",
            "‚úÖ Exists: 1522767129.7038.mp4\n",
            "‚úÖ Exists: 23547.mp4\n",
            "‚úÖ Exists: 23949.mp4\n",
            "‚úÖ Exists: 48823.mp4\n",
            "‚úÖ Exists: 6400.mp4\n",
            "‚úÖ Exists: mskmnqYYsGw.mp4\n",
            "\n",
            "‚û° Gloss: none\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  14%|‚ñà‚ñç        | 14/99 [00:03<00:24,  3.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: none.mp4\n",
            "‚úÖ Exists: 245920.mp4\n",
            "‚úÖ Exists: 1468719394.851.mp4\n",
            "‚úÖ Exists: 7811.mp4\n",
            "\n",
            "‚û° Gloss: noon\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  15%|‚ñà‚ñå        | 15/99 [00:03<00:23,  3.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: noon.mp4\n",
            "‚úÖ Exists: SignSchool Noon-Eu4HtwwjVwk.mp4\n",
            "‚úÖ Exists: SignSchool Noon-mCa1E6MwGMA.mp4\n",
            "‚úÖ Exists: 1468719459.9712.mp4\n",
            "‚úÖ Exists: 6997.mp4\n",
            "\n",
            "‚û° Gloss: north\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  16%|‚ñà‚ñå        | 16/99 [00:04<00:22,  3.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: 49001.mp4\n",
            "‚úÖ Exists: north.mp4\n",
            "‚úÖ Exists: SignSchool North-J_4pPcg2bAk.mp4\n",
            "‚úÖ Exists: 1468719562.50.mp4\n",
            "‚úÖ Exists: 8805.mp4\n",
            "\n",
            "‚û° Gloss: not\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  17%|‚ñà‚ñã        | 17/99 [00:04<00:20,  3.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: 92963.mp4\n",
            "‚úÖ Exists: not.mp4\n",
            "‚úÖ Exists: SignSchool Not-BCUwejC9ZMw.mp4\n",
            "‚úÖ Exists: SignSchool Not-WHFdDSSBcrg.mp4\n",
            "‚úÖ Exists: 1468719771.6543.mp4\n",
            "‚úÖ Exists: 14591.mp4\n",
            "\n",
            "‚û° Gloss: now\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  18%|‚ñà‚ñä        | 18/99 [00:04<00:23,  3.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: 1_jXxjd0lKE.mp4\n",
            "‚úÖ Exists: now.mp4\n",
            "‚úÖ Exists: SignSchool now-0wcuocGAvNU.mp4\n",
            "‚úÖ Exists: bq-HmgjGzmw.mp4\n",
            "‚úÖ Exists: vmGh2IPdSRU.mp4\n",
            "‚úÖ Exists: 7774.mp4\n",
            "‚úÖ Exists: 9785.mp4\n",
            "‚úÖ Exists: EtW1-NdM5qI.mp4\n",
            "‚úÖ Exists: g2j70neQ_lI.mp4\n",
            "‚úÖ Exists: 111145.mp4\n",
            "‚úÖ Exists: OdOx4qaYlKI.mp4\n",
            "‚úÖ Exists: 49006.mp4\n",
            "‚úÖ Exists: 1468719996.5074.mp4\n",
            "‚úÖ Exists: 7687.mp4\n",
            "\n",
            "‚û° Gloss: nurse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  19%|‚ñà‚ñâ        | 19/99 [00:05<00:24,  3.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: 6403.mp4\n",
            "‚úÖ Exists: 49008.mp4\n",
            "‚úÖ Exists: nurse.mp4\n",
            "‚úÖ Exists: SignSchool Nurse-9PdisRY9NCw.mp4\n",
            "‚úÖ Exists: 1468720349.7460.mp4\n",
            "\n",
            "‚û° Gloss: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  20%|‚ñà‚ñà        | 20/99 [00:05<00:23,  3.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: off.mp4\n",
            "‚úÖ Exists: 63624.mp4\n",
            "‚úÖ Exists: 1468721061.9926.mp4\n",
            "‚úÖ Exists: 14404.mp4\n",
            "\n",
            "‚û° Gloss: office\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing glosses 301-399:  22%|‚ñà‚ñà‚ñè       | 22/99 [00:05<00:18,  4.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: 1468721107.3632.mp4\n",
            "‚úÖ Exists: 24017.mp4\n",
            "‚úÖ Exists: 24018.mp4\n",
            "‚úÖ Exists: OFFICE-1549.mp4\n",
            "‚úÖ Exists: 63553.mp4\n",
            "‚úÖ Exists: office.mp4\n",
            "‚úÖ Exists: SignSchool Office opposite-mKds_4EoIUI.mp4\n",
            "‚úÖ Exists: SignSchool Office.mp4\n",
            "\n",
            "‚û° Gloss: ok\n",
            "‚úÖ Exists: 1468721247.921.mp4\n",
            "‚úÖ Exists: 49162.mp4\n",
            "‚úÖ Exists: ok3.mp4\n",
            "‚úÖ Exists: SignSchool OK-Qb4Pt5lG6ho.mp4\n",
            "‚úÖ Exists: 9249.mp4\n",
            "\n",
            "‚û° Gloss: old\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing glosses 301-399:  24%|‚ñà‚ñà‚ñç       | 24/99 [00:06<00:17,  4.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: old.mp4\n",
            "‚úÖ Exists: 49163.mp4\n",
            "‚úÖ Exists: 6337.mp4\n",
            "‚úÖ Exists: SignSchool Old-PYeMIH_-Kog.mp4\n",
            "‚úÖ Exists: 1468721358.1844.mp4\n",
            "\n",
            "‚û° Gloss: onion\n",
            "‚úÖ Exists: 8081.mp4\n",
            "‚úÖ Exists: 8082.mp4\n",
            "‚úÖ Exists: 91532.mp4\n",
            "‚úÖ Exists: onion.mp4\n",
            "‚úÖ Exists: SignSchool Onion-N7uylRlFIWc.mp4\n",
            "‚úÖ Exists: SignSchool Onion.mp4\n",
            "‚úÖ Exists: 1468721992.3631.mp4\n",
            "\n",
            "‚û° Gloss: orange\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  25%|‚ñà‚ñà‚ñå       | 25/99 [00:06<00:17,  4.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: K1VPkuTMpP0.mp4\n",
            "‚úÖ Exists: orange.mp4\n",
            "‚úÖ Exists: pc0-gVEETVg.mp4\n",
            "‚úÖ Exists: WprUBqi3iBc.mp4\n",
            "‚úÖ Exists: WuwIz-vDMUc.mp4\n",
            "‚úÖ Exists: 91534.mp4\n",
            "‚úÖ Exists: SignSchool Orange-mylSeGckZJs.mp4\n",
            "‚úÖ Exists: 1468722445.5887.mp4\n",
            "‚úÖ Exists: 1468722468.1582.mp4\n",
            "‚úÖ Exists: 14519.mp4\n",
            "‚úÖ Exists: 3VAeP_x9tFg.mp4\n",
            "‚úÖ Exists: gHyJ2zdCVHw.mp4\n",
            "‚úÖ Exists: ORANGE-521.mp4\n",
            "‚úÖ Exists: V3hT7-PIJz4.mp4\n",
            "\n",
            "‚û° Gloss: order\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  26%|‚ñà‚ñà‚ñã       | 26/99 [00:06<00:17,  4.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: 1468722551.8765.mp4\n",
            "‚úÖ Exists: 1468722573.1848.mp4\n",
            "‚úÖ Exists: 14253.mp4\n",
            "‚úÖ Exists: 14680.mp4\n",
            "‚úÖ Exists: 457187.mp4\n",
            "‚úÖ Exists: 14688.mp4\n",
            "\n",
            "‚û° Gloss: paint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  27%|‚ñà‚ñà‚ñã       | 27/99 [00:06<00:16,  4.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: zHd4YEFzpwE.mp4\n",
            "‚úÖ Exists: VPeBrHewtSY.mp4\n",
            "‚úÖ Exists: 14823.mp4\n",
            "‚úÖ Exists: 14824.mp4\n",
            "‚úÖ Exists: 9V_pXf6tP_U.mp4\n",
            "‚úÖ Exists: iZ713O5fNos.mp4\n",
            "‚úÖ Exists: SignSchool Paint 2-na0od_u1s8k.mp4\n",
            "‚úÖ Exists: WXDexpGu-zU.mp4\n",
            "‚úÖ Exists: SignSchool Paint-e962KC5Cm5w.mp4\n",
            "‚úÖ Exists: SignSchool Paint-qTA0nc4WYOc.mp4\n",
            "‚úÖ Exists: 1468723124.3329.mp4\n",
            "\n",
            "‚û° Gloss: pants\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  28%|‚ñà‚ñà‚ñä       | 28/99 [00:07<00:15,  4.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: PANTS-871.mp4\n",
            "‚úÖ Exists: 6347.mp4\n",
            "‚úÖ Exists: 6350.mp4\n",
            "‚úÖ Exists: pants.mp4\n",
            "‚úÖ Exists: SignSchool Pants 2-GG4Pexh2M70.mp4\n",
            "‚úÖ Exists: SignSchool Pants-hiYuxvoKVxE.mp4\n",
            "‚úÖ Exists: 1468723514.3164.mp4\n",
            "\n",
            "‚û° Gloss: paper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  29%|‚ñà‚ñà‚ñâ       | 29/99 [00:07<00:16,  4.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: zHd4YEFzpwE.mp4\n",
            "‚úÖ Exists: hjS0dQDgbjo.mp4\n",
            "‚úÖ Exists: LXLQyvK23hc.mp4\n",
            "‚úÖ Exists: 1522767936.333.mp4\n",
            "‚úÖ Exists: 5899.mp4\n",
            "‚úÖ Exists: zNSQm23fGzI.mp4\n",
            "‚úÖ Exists: 49171.mp4\n",
            "‚úÖ Exists: ZrdOYM36B9I.mp4\n",
            "‚úÖ Exists: paper.mp4\n",
            "‚úÖ Exists: SignSchool Paper 2-wnfSW4-iXHY.mp4\n",
            "‚úÖ Exists: SignSchool Paper.mp4\n",
            "‚úÖ Exists: 1468723544.417.mp4\n",
            "\n",
            "‚û° Gloss: party\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  30%|‚ñà‚ñà‚ñà       | 30/99 [00:07<00:15,  4.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: SignSchool Party-OZ1Ap6C6tVk.mp4\n",
            "‚úÖ Exists: SignSchool Play-TvZ8pfNOX2U.mp4\n",
            "‚úÖ Exists: 1468724131.6734.mp4\n",
            "‚úÖ Exists: PARTY-875.mp4\n",
            "‚úÖ Exists: 457609.mp4\n",
            "‚úÖ Exists: 6693.mp4\n",
            "‚úÖ Exists: 6694.mp4\n",
            "‚úÖ Exists: party.mp4\n",
            "\n",
            "‚û° Gloss: past\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  31%|‚ñà‚ñà‚ñà‚ñè      | 31/99 [00:07<00:15,  4.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: 5971.mp4\n",
            "‚úÖ Exists: 6164.mp4\n",
            "‚úÖ Exists: 9194.mp4\n",
            "‚úÖ Exists: 157905.mp4\n",
            "‚úÖ Exists: SignSchool Last-0SLfkAHELAs.mp4\n",
            "‚úÖ Exists: 1466681314.2550.mp4\n",
            "‚úÖ Exists: 1468724279.1587.mp4\n",
            "\n",
            "‚û° Gloss: patient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/99 [00:08<00:15,  4.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: PATIENT-250.mp4\n",
            "‚úÖ Exists: 1468724395.2789.mp4\n",
            "‚úÖ Exists: patient.mp4\n",
            "‚úÖ Exists: 8600.mp4\n",
            "‚úÖ Exists: SignSchool Patient copy-ZD4Qp4w9Wwo.mp4\n",
            "‚úÖ Exists: SignSchool Patient-V-R54zsyHss.mp4\n",
            "\n",
            "‚û° Gloss: pay\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/99 [00:08<00:15,  4.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: pay.mp4\n",
            "‚úÖ Exists: SignSchool Pay-Mw8SMIRky0M.mp4\n",
            "‚úÖ Exists: 1468724496.7165.mp4\n",
            "‚úÖ Exists: 14618.mp4\n",
            "‚úÖ Exists: PAY-878.mp4\n",
            "‚úÖ Exists: 6920.mp4\n",
            "\n",
            "‚û° Gloss: pencil\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  34%|‚ñà‚ñà‚ñà‚ñç      | 34/99 [00:08<00:16,  3.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: 457714.mp4\n",
            "‚úÖ Exists: 1468724738.1474.mp4\n",
            "‚úÖ Exists: 1522768045.488.mp4\n",
            "‚úÖ Exists: 8972.mp4\n",
            "\n",
            "‚û° Gloss: people\n",
            "‚úÖ Exists: people.mp4\n",
            "‚úÖ Exists: 49185.mp4\n",
            "‚úÖ Exists: SignSchool People-fgoK5-CEOzU.mp4\n",
            "‚úÖ Exists: 1525610734.9912.mp4\n",
            "‚úÖ Exists: 6404.mp4\n",
            "\n",
            "‚û° Gloss: pepper\n",
            "‚úÖ Exists: SignSchool Pepper, Shake-5j8GlxSR9aA.mp4\n",
            "‚úÖ Exists: 1468724909.6782.mp4\n",
            "‚úÖ Exists: 8000.mp4\n",
            "‚úÖ Exists: 91562.mp4\n",
            "‚úÖ Exists: pepper.mp4\n",
            "‚úÖ Exists: SignSchool Pepper, Shake 2-IsbF6PZcOsU.mp4\n",
            "‚úÖ Exists: SignSchool Pepper, Shake 3-RblFCcMkvmQ.mp4\n",
            "\n",
            "‚û° Gloss: person\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing glosses 301-399:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/99 [00:09<00:10,  5.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: 1468725222.7589.mp4\n",
            "‚úÖ Exists: 22125.mp4\n",
            "‚úÖ Exists: PERSON-888.mp4\n",
            "‚úÖ Exists: 457738.mp4\n",
            "‚úÖ Exists: SignSchool Person-JuPDRv_OWWQ.mp4\n",
            "\n",
            "‚û° Gloss: perspective\n",
            "‚úÖ Exists: SignSchool Perspective-JokfTNsMNoU.mp4\n",
            "‚úÖ Exists: SignSchool Point of View, Perspective, Paradigm Shift-NZwY21cyXEk.mp4\n",
            "‚úÖ Exists: 1499870482.2044.mp4\n",
            "‚úÖ Exists: 14270.mp4\n",
            "‚úÖ Exists: 14332.mp4\n",
            "‚úÖ Exists: 14333.mp4\n",
            "‚úÖ Exists: 62881.mp4\n",
            "\n",
            "‚û° Gloss: phone\n",
            "‚úÖ Exists: 457751.mp4\n",
            "‚úÖ Exists: 457752.mp4\n",
            "‚úÖ Exists: 1468725580.1235.mp4\n",
            "‚úÖ Exists: 7254.mp4\n",
            "\n",
            "‚û° Gloss: picture\n",
            "‚úÖ Exists: 63560.mp4\n",
            "‚úÖ Exists: picture.mp4\n",
            "‚úÖ Exists: SignSchool Image, Picture-qmGvlGKLGeI.mp4\n",
            "‚úÖ Exists: 1468726080.6503.mp4\n",
            "‚úÖ Exists: 5976.mp4\n",
            "\n",
            "‚û° Gloss: pink\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 41/99 [00:09<00:07,  7.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: zHd4YEFzpwE.mp4\n",
            "‚úÖ Exists: pink.mp4\n",
            "‚úÖ Exists: 2igUp9Fvq-E.mp4\n",
            "‚úÖ Exists: WprUBqi3iBc.mp4\n",
            "‚úÖ Exists: SignSchool Pink var.mp4\n",
            "‚úÖ Exists: SignSchool Pink.mp4\n",
            "‚úÖ Exists: 1468726359.6744.mp4\n",
            "‚úÖ Exists: 22812.mp4\n",
            "‚úÖ Exists: aNilWTBIl0g.mp4\n",
            "‚úÖ Exists: PINK-903.mp4\n",
            "‚úÖ Exists: 49192.mp4\n",
            "‚úÖ Exists: S8k8gjRdYXw.mp4\n",
            "\n",
            "‚û° Gloss: pizza\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing glosses 301-399:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/99 [00:09<00:08,  6.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: zHd4YEFzpwE.mp4\n",
            "‚úÖ Exists: pizza.mp4\n",
            "‚úÖ Exists: XIZ2DrdEU3k.mp4\n",
            "‚úÖ Exists: pizza-2.mp4\n",
            "‚úÖ Exists: SignSchool Pizza 2-Gm8PTVKHF1s.mp4\n",
            "‚úÖ Exists: SignSchool Pizza-mJVC9oI13Lo.mp4\n",
            "‚úÖ Exists: PIZZA-904.mp4\n",
            "‚úÖ Exists: 1468726411.9058.mp4\n",
            "‚úÖ Exists: 457790.mp4\n",
            "‚úÖ Exists: 14610.mp4\n",
            "‚úÖ Exists: 6671.mp4\n",
            "‚úÖ Exists: 6673.mp4\n",
            "‚úÖ Exists: JZeO2EXjxOE.mp4\n",
            "‚úÖ Exists: nrXA-R3VgJ8.mp4\n",
            "\n",
            "‚û° Gloss: plan\n",
            "‚úÖ Exists: 14686.mp4\n",
            "‚úÖ Exists: 49197.mp4\n",
            "‚úÖ Exists: SignSchool Plan-8OhwY0hO-_I.mp4\n",
            "‚úÖ Exists: 1468726461.868.mp4\n",
            "‚úÖ Exists: 14678.mp4\n",
            "\n",
            "‚û° Gloss: play\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 44/99 [00:10<00:09,  5.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: play.mp4\n",
            "‚úÖ Exists: 116328.mp4\n",
            "‚úÖ Exists: M80w0mda7zM.mp4\n",
            "‚úÖ Exists: 1468726646.6390.mp4\n",
            "‚úÖ Exists: 1468726677.3753.mp4\n",
            "‚úÖ Exists: 14657.mp4\n",
            "‚úÖ Exists: 6372.mp4\n",
            "‚úÖ Exists: ODByBNyosxc.mp4\n",
            "‚úÖ Exists: SignSchool Play, Algeria-31fxkHUhB_k.mp4\n",
            "‚úÖ Exists: SignSchool Play-TvZ8pfNOX2U.mp4\n",
            "\n",
            "‚û° Gloss: please\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 45/99 [00:10<00:11,  4.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: please.mp4\n",
            "‚úÖ Exists: SignSchool Please-eyu0V3s1-XA.mp4\n",
            "‚úÖ Exists: 1468754067.6864.mp4\n",
            "‚úÖ Exists: 1522767174.1064.mp4\n",
            "‚úÖ Exists: 6966.mp4\n",
            "\n",
            "‚û° Gloss: plus\n",
            "‚úÖ Exists: PLUS-909.mp4\n",
            "‚úÖ Exists: 116333.mp4\n",
            "‚úÖ Exists: SignSchool Plus-bfY_3UCUOxE.mp4\n",
            "‚úÖ Exists: SignSchool Pro-ckIRGIIvnjo.mp4\n",
            "‚úÖ Exists: 1499870520.9537.mp4\n",
            "‚úÖ Exists: 6474.mp4\n",
            "\n",
            "‚û° Gloss: point\n",
            "‚úÖ Exists: point.mp4\n",
            "‚úÖ Exists: SignSchool Point-Q6UwBpW4YGQ.mp4\n",
            "‚úÖ Exists: SignSchool To the Point-9xtUROYOGFM.mp4\n",
            "‚úÖ Exists: 1468754235.9169.mp4\n",
            "‚úÖ Exists: 58395.mp4\n",
            "‚úÖ Exists: 14332.mp4\n",
            "\n",
            "‚û° Gloss: police\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/99 [00:10<00:07,  7.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: 14010.mp4\n",
            "‚úÖ Exists: 14012.mp4\n",
            "‚úÖ Exists: POLICE-912.mp4\n",
            "‚úÖ Exists: 49201.mp4\n",
            "‚úÖ Exists: SignSchool Cop 2-Nba4p7eNbnY.mp4\n",
            "‚úÖ Exists: SignSchool Officer-_bWGe-DFJdw.mp4\n",
            "‚úÖ Exists: 1468754360.5475.mp4\n",
            "\n",
            "‚û° Gloss: poor\n",
            "‚úÖ Exists: 49208.mp4\n",
            "‚úÖ Exists: poor.mp4\n",
            "‚úÖ Exists: SignSchool Poor%2C Poverty.mp4\n",
            "‚úÖ Exists: 1468754688.3498.mp4\n",
            "‚úÖ Exists: 2404.mp4\n",
            "\n",
            "‚û° Gloss: possible\n",
            "‚úÖ Exists: 9066.mp4\n",
            "‚úÖ Exists: 9565.mp4\n",
            "‚úÖ Exists: 9566.mp4\n",
            "‚úÖ Exists: POSSIBLE-2041.mp4\n",
            "‚úÖ Exists: 51138.mp4\n",
            "‚úÖ Exists: 1468754886.902.mp4\n",
            "‚úÖ Exists: 7818.mp4\n",
            "\n",
            "‚û° Gloss: potato\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing glosses 301-399:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 52/99 [00:11<00:06,  7.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: POTATO-922.mp4\n",
            "‚úÖ Exists: 485648.mp4\n",
            "‚úÖ Exists: potato.mp4\n",
            "‚úÖ Exists: SignSchool Potato-bMnw7FVrglA.mp4\n",
            "‚úÖ Exists: SignSchool Potato-wbgrLPR3DX4.mp4\n",
            "‚úÖ Exists: 1468754996.9439.mp4\n",
            "‚úÖ Exists: 23006.mp4\n",
            "\n",
            "‚û° Gloss: practice\n",
            "‚úÖ Exists: 8953.mp4\n",
            "‚úÖ Exists: 9822.mp4\n",
            "‚úÖ Exists: SignSchool Practice-R0c75LRKKQ8.mp4\n",
            "‚úÖ Exists: 1468755145.5671.mp4\n",
            "‚úÖ Exists: 1522769215.9095.mp4\n",
            "‚úÖ Exists: 23172.mp4\n",
            "‚úÖ Exists: 23173.mp4\n",
            "\n",
            "‚û° Gloss: president\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 53/99 [00:11<00:06,  6.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: 22288.mp4\n",
            "‚úÖ Exists: 22289.mp4\n",
            "‚úÖ Exists: PRESIDENT-926.mp4\n",
            "‚úÖ Exists: 485649.mp4\n",
            "‚úÖ Exists: president.mp4\n",
            "‚úÖ Exists: SignSchool President-IKq3CCMS0YI.mp4\n",
            "‚úÖ Exists: 1468755502.8846.mp4\n",
            "\n",
            "‚û° Gloss: problem\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 54/99 [00:11<00:07,  5.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: 1468756235.8420.mp4\n",
            "‚úÖ Exists: 6659.mp4\n",
            "‚úÖ Exists: 6661.mp4\n",
            "‚úÖ Exists: 63567.mp4\n",
            "‚úÖ Exists: SignSchool Problem copy-H6BM_-Y6GF8.mp4\n",
            "‚úÖ Exists: SignSchool Problem-cDVI4LelnNc.mp4\n",
            "\n",
            "‚û° Gloss: pull\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 55/99 [00:11<00:08,  5.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: zHd4YEFzpwE.mp4\n",
            "‚úÖ Exists: pull.mp4\n",
            "‚úÖ Exists: Z25ng9maolE.mp4\n",
            "‚úÖ Exists: 1468757283.4828.mp4\n",
            "‚úÖ Exists: 22921.mp4\n",
            "‚úÖ Exists: i5qAkbXqz9Y.mp4\n",
            "‚úÖ Exists: kO_5FgRTVCs.mp4\n",
            "‚úÖ Exists: PULL-229.mp4\n",
            "‚úÖ Exists: 306777.mp4\n",
            "‚úÖ Exists: zfNH9EdhgNo.mp4\n",
            "‚úÖ Exists: SignSchool Pull2-3gtcnXxKZHs.mp4\n",
            "‚úÖ Exists: SignSchool Pull-78bHeANdyYc.mp4\n",
            "‚úÖ Exists: SignSchool Pull-N_bMKvAGxjs.mp4\n",
            "\n",
            "‚û° Gloss: purple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 56/99 [00:12<00:09,  4.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: zHd4YEFzpwE.mp4\n",
            "‚úÖ Exists: purple.mp4\n",
            "‚úÖ Exists: ESK6NAvvsKg.mp4\n",
            "‚úÖ Exists: WprUBqi3iBc.mp4\n",
            "‚úÖ Exists: 6858.mp4\n",
            "‚úÖ Exists: lKzYzA2JZaI.mp4\n",
            "‚úÖ Exists: zgdzYtHB6jg.mp4\n",
            "‚úÖ Exists: PURPLE-989.mp4\n",
            "‚úÖ Exists: 49224.mp4\n",
            "‚úÖ Exists: SignSchool Purple.mp4\n",
            "‚úÖ Exists: 1468757349.9136.mp4\n",
            "\n",
            "‚û° Gloss: quiet\n",
            "‚úÖ Exists: 5611.mp4\n",
            "‚úÖ Exists: QUIET-233.mp4\n",
            "‚úÖ Exists: 49264.mp4\n",
            "‚úÖ Exists: quiet.mp4\n",
            "‚úÖ Exists: SignSchool Silent, Quiet-9rp2ICTeDgM.mp4\n",
            "‚úÖ Exists: 1468757731.3435.mp4\n",
            "\n",
            "‚û° Gloss: rabbit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 58/99 [00:12<00:07,  5.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: Qc7Dc_qF5NI.mp4\n",
            "‚úÖ Exists: 457813.mp4\n",
            "‚úÖ Exists: SignSchool Rabbit, Bunny 2-WoWke5dhzfs.mp4\n",
            "‚úÖ Exists: SignSchool Rabbit, Bunny-P7l2qQaGw5A.mp4\n",
            "‚úÖ Exists: 1468757899.8382.mp4\n",
            "‚úÖ Exists: 24029.mp4\n",
            "‚úÖ Exists: 24030.mp4\n",
            "‚úÖ Exists: 89qJlLBkRho.mp4\n",
            "‚úÖ Exists: v4v20gVPJyU.mp4\n",
            "‚úÖ Exists: 457812.mp4\n",
            "\n",
            "‚û° Gloss: rain\n",
            "‚úÖ Exists: 49266.mp4\n",
            "‚úÖ Exists: rain.mp4\n",
            "‚úÖ Exists: SignSchool Rain.mp4\n",
            "‚úÖ Exists: 1468758059.4199.mp4\n",
            "‚úÖ Exists: 22993.mp4\n",
            "\n",
            "‚û° Gloss: read\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 60/99 [00:12<00:06,  6.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: Qc7Dc_qF5NI.mp4\n",
            "‚úÖ Exists: Nx5yZfVGuX8.mp4\n",
            "‚úÖ Exists: WGfiiDgrq1I.mp4\n",
            "‚úÖ Exists: ybYuTigZNQo.mp4\n",
            "‚úÖ Exists: Ae98aQIiVZo.mp4\n",
            "‚úÖ Exists: 457816.mp4\n",
            "‚úÖ Exists: read.mp4\n",
            "‚úÖ Exists: SignSchool Read-ONxA42RnRps.mp4\n",
            "‚úÖ Exists: 1468758340.1852.mp4\n",
            "‚úÖ Exists: 7042.mp4\n",
            "\n",
            "‚û° Gloss: ready\n",
            "‚úÖ Exists: 158011.mp4\n",
            "‚úÖ Exists: 49268.mp4\n",
            "‚úÖ Exists: SignSchool Ready 2h-Mia86oYwZKs.mp4\n",
            "‚úÖ Exists: 1468758376.4036.mp4\n",
            "‚úÖ Exists: 1522768074.9713.mp4\n",
            "‚úÖ Exists: 14681.mp4\n",
            "‚úÖ Exists: READY-2161.mp4\n",
            "‚úÖ Exists: 9415.mp4\n",
            "‚úÖ Exists: 9416.mp4\n",
            "\n",
            "‚û° Gloss: red\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing glosses 301-399:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 63/99 [00:13<00:05,  6.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: Qc7Dc_qF5NI.mp4\n",
            "‚úÖ Exists: red.mp4\n",
            "‚úÖ Exists: WprUBqi3iBc.mp4\n",
            "‚úÖ Exists: 6859.mp4\n",
            "‚úÖ Exists: EyKmo6RXVUU.mp4\n",
            "‚úÖ Exists: pyW0OYYrZ7U.mp4\n",
            "‚úÖ Exists: 49271.mp4\n",
            "‚úÖ Exists: SignSchool Red.mp4\n",
            "‚úÖ Exists: 1468758865.1287.mp4\n",
            "\n",
            "‚û° Gloss: remember\n",
            "‚úÖ Exists: 1468759686.1526.mp4\n",
            "‚úÖ Exists: 9114.mp4\n",
            "‚úÖ Exists: REMEMBER-958.mp4\n",
            "‚úÖ Exists: 49277.mp4\n",
            "‚úÖ Exists: SignSchool Remember 2-NHgRSDhylOA.mp4\n",
            "‚úÖ Exists: SignSchool Remember-gTlLekzIsEY.mp4\n",
            "\n",
            "‚û° Gloss: research\n",
            "‚úÖ Exists: 23210.mp4\n",
            "‚úÖ Exists: SignSchool Research, Investigate-gIjCOB6HpNo.mp4\n",
            "‚úÖ Exists: SignSchool Research-GEG5sDkVTBY.mp4\n",
            "‚úÖ Exists: 1468760160.6145.mp4\n",
            "\n",
            "‚û° Gloss: restaurant\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/99 [00:13<00:05,  6.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: 22673.mp4\n",
            "‚úÖ Exists: RESTAURANT-969.mp4\n",
            "‚úÖ Exists: 50756.mp4\n",
            "‚úÖ Exists: restaurant.mp4\n",
            "‚úÖ Exists: SignSchool Restaurant 2-uv8d_492XI0.mp4\n",
            "‚úÖ Exists: 1468760497.4726.mp4\n",
            "\n",
            "‚û° Gloss: ride\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 66/99 [00:13<00:05,  5.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: 23884.mp4\n",
            "‚úÖ Exists: 24005.mp4\n",
            "‚úÖ Exists: SignSchool Ride-a35RcUi-rxk.mp4\n",
            "‚úÖ Exists: SignSchool Ride-huEE3RXyL-0.mp4\n",
            "‚úÖ Exists: 1468760986.966.mp4\n",
            "\n",
            "‚û° Gloss: right\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 67/99 [00:13<00:06,  5.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: e1RF5h4OgOg.mp4\n",
            "‚úÖ Exists: right.mp4\n",
            "‚úÖ Exists: SignSchool Right, Correct-XhAAfS8YF4o.mp4\n",
            "‚úÖ Exists: SignSchool Right-VC1zsuX_WU4.mp4\n",
            "‚úÖ Exists: 1466903512.8429.mp4\n",
            "‚úÖ Exists: 1468761005.7507.mp4\n",
            "‚úÖ Exists: 22624.mp4\n",
            "‚úÖ Exists: 6234.mp4\n",
            "‚úÖ Exists: 5UnUWDYxfv0.mp4\n",
            "‚úÖ Exists: mxX9mVMl8O4.mp4\n",
            "\n",
            "‚û° Gloss: room\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 68/99 [00:14<00:06,  4.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: Qc7Dc_qF5NI.mp4\n",
            "‚úÖ Exists: room.mp4\n",
            "‚úÖ Exists: SignSchool Room, Box-md_GwEQC3G8.mp4\n",
            "‚úÖ Exists: SignSchool Room-c_wEkcZ2eqU.mp4\n",
            "‚úÖ Exists: SignSchool room-eFVfYZffYLo.mp4\n",
            "‚úÖ Exists: SignSchool Room 2.mp4\n",
            "‚úÖ Exists: 1468761369.8817.mp4\n",
            "‚úÖ Exists: 9515.mp4\n",
            "‚úÖ Exists: 9516.mp4\n",
            "‚úÖ Exists: ByET2PW3OrQ.mp4\n",
            "\n",
            "‚û° Gloss: run\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing glosses 301-399:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 70/99 [00:14<00:06,  4.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: 50771.mp4\n",
            "‚úÖ Exists: eLu4zVJhTqU.mp4\n",
            "‚úÖ Exists: run.mp4\n",
            "‚úÖ Exists: jPGs28X_O54.mp4\n",
            "‚úÖ Exists: T0_ts-_j8w0.mp4\n",
            "‚úÖ Exists: SignSchool run 2.mp4\n",
            "‚úÖ Exists: 1468761794.9611.mp4\n",
            "‚úÖ Exists: 6445.mp4\n",
            "‚úÖ Exists: 6446.mp4\n",
            "\n",
            "‚û° Gloss: russia\n",
            "‚úÖ Exists: SignSchool Russia 2-74OAafnvKs0.mp4\n",
            "‚úÖ Exists: SignSchool Russia 3-6S4H3Nl14Dw.mp4\n",
            "‚úÖ Exists: SignSchool Russia-xJKc-LqcgFA.mp4\n",
            "‚úÖ Exists: 1512918340.9089.mp4\n",
            "‚úÖ Exists: 7790.mp4\n",
            "‚úÖ Exists: 7791.mp4\n",
            "‚úÖ Exists: 455370.mp4\n",
            "\n",
            "‚û° Gloss: sad\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 71/99 [00:14<00:05,  4.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: sad.mp4\n",
            "‚úÖ Exists: 1468762255.9874.mp4\n",
            "‚úÖ Exists: 7167.mp4\n",
            "‚úÖ Exists: 50772.mp4\n",
            "‚úÖ Exists: SignSchool Sad 2-v80n5JTnyTQ.mp4\n",
            "‚úÖ Exists: SignSchool Sad-GCyXSsozH0o.mp4\n",
            "\n",
            "‚û° Gloss: salt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 72/99 [00:15<00:06,  4.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: salt.mp4\n",
            "‚úÖ Exists: 9688.mp4\n",
            "‚úÖ Exists: SALT-2012.mp4\n",
            "‚úÖ Exists: 457619.mp4\n",
            "‚úÖ Exists: SignSchool Salt-n9HhHHFm8A0.mp4\n",
            "‚úÖ Exists: 1468762614.6611.mp4\n",
            "\n",
            "‚û° Gloss: same\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/99 [00:15<00:06,  4.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: 50775.mp4\n",
            "‚úÖ Exists: SignSchool Same, Similar-u52CAu7ygrY.mp4\n",
            "‚úÖ Exists: SignSchool Same-DeqpnWES_bs.mp4\n",
            "‚úÖ Exists: 1468762687.6352.mp4\n",
            "‚úÖ Exists: 14298.mp4\n",
            "‚úÖ Exists: 8807.mp4\n",
            "‚úÖ Exists: 8808.mp4\n",
            "‚úÖ Exists: cnX45VWbjmc.mp4\n",
            "‚úÖ Exists: o1NMZAc7vO4.mp4\n",
            "‚úÖ Exists: OwtSP1QozuE.mp4\n",
            "‚úÖ Exists: SignSchool Same 2-YS_M2kWlN4E.mp4\n",
            "\n",
            "‚û° Gloss: sandwich\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 74/99 [00:15<00:06,  4.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: 14116.mp4\n",
            "‚úÖ Exists: 7885.mp4\n",
            "‚úÖ Exists: 9082.mp4\n",
            "‚úÖ Exists: SANDWICH-993.mp4\n",
            "‚úÖ Exists: SANDWICH-994.mp4\n",
            "‚úÖ Exists: 485651.mp4\n",
            "‚úÖ Exists: sandwich.mp4\n",
            "‚úÖ Exists: SignSchool Sandwich.mp4\n",
            "‚úÖ Exists: 1468762862.7837.mp4\n",
            "\n",
            "‚û° Gloss: saturday\n",
            "‚úÖ Exists: 6023.mp4\n",
            "‚úÖ Exists: 8984.mp4\n",
            "‚úÖ Exists: SATURDAY-997.mp4\n",
            "‚úÖ Exists: saturday.mp4\n",
            "‚úÖ Exists: SignSchool Saturday-rr2mkJCxX2g.mp4\n",
            "‚úÖ Exists: 1468762994.5904.mp4\n",
            "\n",
            "‚û° Gloss: save\n",
            "‚úÖ Exists: 22978.mp4\n",
            "‚úÖ Exists: 22979.mp4\n",
            "‚úÖ Exists: 8775.mp4\n",
            "‚úÖ Exists: 482569.mp4\n",
            "‚úÖ Exists: SignSchool Saving-9ZSHxT8UfRg.mp4\n",
            "‚úÖ Exists: 1468763049.7173.mp4\n",
            "\n",
            "‚û° Gloss: scared\n",
            "‚úÖ Exists: 7213.mp4\n",
            "‚úÖ Exists: 63572.mp4\n",
            "‚úÖ Exists: scared.mp4\n",
            "‚úÖ Exists: SignSchool Scared-DrJ4IBnI_Jo.mp4\n",
            "‚úÖ Exists: 1468763300.273.mp4\n",
            "‚úÖ Exists: 5943.mp4\n",
            "\n",
            "‚û° Gloss: school\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 78/99 [00:16<00:03,  6.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exists: XB3Q6IX6CBo.mp4\n",
            "‚úÖ Exists: school.mp4\n",
            "‚úÖ Exists: hjS0dQDgbjo.mp4\n",
            "‚úÖ Exists: ig7SKr3tvWc.mp4\n",
            "‚úÖ Exists: 6564.mp4\n",
            "‚úÖ Exists: vuE3RN10AGA.mp4\n",
            "‚úÖ Exists: 50778.mp4\n",
            "‚úÖ Exists: SignSchool Academic, School-ZsoedvSYMpA.mp4\n",
            "‚úÖ Exists: SignSchool School.mp4\n",
            "‚úÖ Exists: 1468763371.5844.mp4\n",
            "‚úÖ Exists: 1522768179.9250.mp4\n",
            "\n",
            "‚û° Gloss: science\n",
            "‚úÖ Exists: 50779.mp4\n",
            "‚úÖ Exists: science.mp4\n",
            "‚úÖ Exists: SignSchool Science-1MJHtOtH2mE.mp4\n",
            "‚úÖ Exists: 1468763393.453.mp4\n",
            "‚úÖ Exists: 6456.mp4\n",
            "\n",
            "‚û° Gloss: scissors\n",
            "‚úÖ Exists: 8668.mp4\n",
            "‚úÖ Exists: 8669.mp4\n",
            "‚úÖ Exists: 310864.mp4\n",
            "‚úÖ Exists: scissors.mp4\n",
            "‚úÖ Exists: SignSchool Scissors-_fm4BdBB7kQ.mp4\n",
            "‚úÖ Exists: 1468763457.6722.mp4\n",
            "\n",
            "‚û° Gloss: score\n",
            "‚úÖ Exists: SignSchool score basketball-h-GkTVSK-BU.mp4\n",
            "‚úÖ Exists: SignSchool score-dSy1PvZ1UhE.mp4\n",
            "‚úÖ Exists: 1468763511.5652.mp4\n",
            "‚úÖ Exists: 382.mp4\n",
            "‚úÖ Exists: 23691.mp4\n",
            "‚úÖ Exists: 87622.mp4\n",
            "‚úÖ Exists: 23767.mp4\n",
            "‚úÖ Exists: SignSchool One-Point-wHshoRFERnQ.mp4\n",
            "\n",
            "‚û° Gloss: secret\n",
            "‚úÖ Exists: 23301.mp4\n",
            "‚úÖ Exists: 23302.mp4\n",
            "‚úÖ Exists: 23304.mp4\n",
            "‚úÖ Exists: SECRET-1009.mp4\n",
            "‚úÖ Exists: 310848.mp4\n",
            "‚úÖ Exists: SignSchool Secret-SGD613coMXA.mp4\n",
            "‚úÖ Exists: 1468764780.3618.mp4\n",
            "\n",
            "‚û° Gloss: secretary\n",
            "üé• Extracting: XB3Q6IX6CBo.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/secretary/XB3Q6IX6CBo.npy\n",
            "üé• Extracting: 51753.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/secretary/51753.npy\n",
            "üé• Extracting: secretary.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/secretary/secretary.npy\n",
            "üé• Extracting: SignSchool Secetary 2-GLOdTEXLaEQ.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/secretary/SignSchool Secetary 2-GLOdTEXLaEQ.npy\n",
            "üé• Extracting: SignSchool Secetary-crVLvTbmEGo.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/secretary/SignSchool Secetary-crVLvTbmEGo.npy\n",
            "üé• Extracting: 1468764756.5155.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/secretary/1468764756.5155.npy\n",
            "üé• Extracting: 6921.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/secretary/6921.npy\n",
            "üé• Extracting: 8440.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/secretary/8440.npy\n",
            "üé• Extracting: SECRETARY-1010.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/secretary/SECRETARY-1010.npy\n",
            "üé• Extracting: 8857.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/secretary/8857.npy\n",
            "üé• Extracting: 7FvR0sUKA84.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/secretary/7FvR0sUKA84.npy\n",
            "üé• Extracting: ASCPx0sjuwM.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/secretary/ASCPx0sjuwM.npy\n",
            "üé• Extracting: D1Zl-7gp0Zs.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/secretary/D1Zl-7gp0Zs.npy\n",
            "üé• Extracting: U3lM2jSLNno.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 83/99 [15:29<25:43, 96.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/secretary/U3lM2jSLNno.npy\n",
            "\n",
            "‚û° Gloss: sentence\n",
            "üé• Extracting: 8611.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sentence/8611.npy\n",
            "üé• Extracting: 8612.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sentence/8612.npy\n",
            "üé• Extracting: sentence.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sentence/sentence.npy\n",
            "üé• Extracting: SignSchool Sentence copy-hibsVZEt1H0.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sentence/SignSchool Sentence copy-hibsVZEt1H0.npy\n",
            "üé• Extracting: SignSchool Sentence-neWsRHnuKc4.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sentence/SignSchool Sentence-neWsRHnuKc4.npy\n",
            "üé• Extracting: 1468765232.9077.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 84/99 [15:58<21:53, 87.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sentence/1468765232.9077.npy\n",
            "\n",
            "‚û° Gloss: shape\n",
            "üé• Extracting: shape.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/shape/shape.npy\n",
            "üé• Extracting: 62885.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/shape/62885.npy\n",
            "üé• Extracting: SignSchool Shape-_9LTBEMaeAg.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/shape/SignSchool Shape-_9LTBEMaeAg.npy\n",
            "üé• Extracting: 1468765730.2524.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/shape/1468765730.2524.npy\n",
            "üé• Extracting: 1546575025.9272.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/shape/1546575025.9272.npy\n",
            "üé• Extracting: 6058.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 85/99 [16:41<18:47, 80.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/shape/6058.npy\n",
            "\n",
            "‚û° Gloss: share\n",
            "üé• Extracting: 6543.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/share/6543.npy\n",
            "üé• Extracting: 8853.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/share/8853.npy\n",
            "üé• Extracting: SHARE-251.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/share/SHARE-251.npy\n",
            "üé• Extracting: SignSchool Share-IaO4TjGuEYw.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/share/SignSchool Share-IaO4TjGuEYw.npy\n",
            "üé• Extracting: SignSchool Share-vz2mlzywHbs.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/share/SignSchool Share-vz2mlzywHbs.npy\n",
            "üé• Extracting: 1468765753.9375.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 86/99 [17:10<15:25, 71.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/share/1468765753.9375.npy\n",
            "\n",
            "‚û° Gloss: shirt\n",
            "üé• Extracting: XB3Q6IX6CBo.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/shirt/XB3Q6IX6CBo.npy\n",
            "üé• Extracting: shirt.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/shirt/shirt.npy\n",
            "üé• Extracting: 50791.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/shirt/50791.npy\n",
            "üé• Extracting: shirt-plaid.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/shirt/shirt-plaid.npy\n",
            "üé• Extracting: shirt-vneck.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/shirt/shirt-vneck.npy\n",
            "üé• Extracting: SignSchool Shirt-Mi3CHINu71M.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/shirt/SignSchool Shirt-Mi3CHINu71M.npy\n",
            "üé• Extracting: 1468766065.7078.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/shirt/1468766065.7078.npy\n",
            "üé• Extracting: 23615.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/shirt/23615.npy\n",
            "üé• Extracting: 23616.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/shirt/23616.npy\n",
            "üé• Extracting: 23618.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/shirt/23618.npy\n",
            "üé• Extracting: 6249.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/shirt/6249.npy\n",
            "üé• Extracting: 7169.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/shirt/7169.npy\n",
            "üé• Extracting: 4ln_5SQd__w.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/shirt/4ln_5SQd__w.npy\n",
            "üé• Extracting: 9Rcg3QJiClc.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/shirt/9Rcg3QJiClc.npy\n",
            "üé• Extracting: i14EM0ZbtUY.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 87/99 [32:31<49:35, 247.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/shirt/i14EM0ZbtUY.npy\n",
            "\n",
            "‚û° Gloss: shoes\n",
            "üé• Extracting: 50804.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/shoes/50804.npy\n",
            "üé• Extracting: shoes.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/shoes/shoes.npy\n",
            "üé• Extracting: SignSchool Shoes-2O1jxmNeMNU.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/shoes/SignSchool Shoes-2O1jxmNeMNU.npy\n",
            "üé• Extracting: 1468766147.3549.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/shoes/1468766147.3549.npy\n",
            "üé• Extracting: 7115.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 88/99 [32:57<36:07, 197.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/shoes/7115.npy\n",
            "\n",
            "‚û° Gloss: shop\n",
            "üé• Extracting: SHOP_2-1039.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/shop/SHOP_2-1039.npy\n",
            "üé• Extracting: 1468766273.8526.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/shop/1468766273.8526.npy\n",
            "üé• Extracting: 1522678509.3333.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/shop/1522678509.3333.npy\n",
            "üé• Extracting: 14628.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/shop/14628.npy\n",
            "üé• Extracting: 6703.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/99 [33:25<25:53, 155.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/shop/6703.npy\n",
            "\n",
            "‚û° Gloss: short\n",
            "üé• Extracting: XB3Q6IX6CBo.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/short/XB3Q6IX6CBo.npy\n",
            "üé• Extracting: bq-HmgjGzmw.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/short/bq-HmgjGzmw.npy\n",
            "üé• Extracting: short.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/short/short.npy\n",
            "üé• Extracting: SignSchool Child-D-7K4sSsMWw.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/short/SignSchool Child-D-7K4sSsMWw.npy\n",
            "üé• Extracting: SignSchool Short-bjSx00_gSOs.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/short/SignSchool Short-bjSx00_gSOs.npy\n",
            "üé• Extracting: 1466724014.5239.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/short/1466724014.5239.npy\n",
            "üé• Extracting: 1468766397.2205.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/short/1468766397.2205.npy\n",
            "üé• Extracting: 1546575090.7731.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/short/1546575090.7731.npy\n",
            "üé• Extracting: SHORT-252.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/short/SHORT-252.npy\n",
            "üé• Extracting: 65914.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/short/65914.npy\n",
            "üé• Extracting: 23534.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/short/23534.npy\n",
            "üé• Extracting: 9191.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/short/9191.npy\n",
            "üé• Extracting: 9192.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 90/99 [58:03<1:14:58, 499.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/short/9192.npy\n",
            "\n",
            "‚û° Gloss: show\n",
            "üé• Extracting: SHOW-254.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/show/SHOW-254.npy\n",
            "üé• Extracting: 4Nh1iFv2BMc.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/show/4Nh1iFv2BMc.npy\n",
            "üé• Extracting: 1468378950.4291.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/show/1468378950.4291.npy\n",
            "üé• Extracting: 1468766573.2319.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/show/1468766573.2319.npy\n",
            "üé• Extracting: 14243.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/show/14243.npy\n",
            "üé• Extracting: show.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/show/show.npy\n",
            "üé• Extracting: 9921.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/show/9921.npy\n",
            "üé• Extracting: 9922.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/show/9922.npy\n",
            "üé• Extracting: 3tywmZLwClQ.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/show/3tywmZLwClQ.npy\n",
            "üé• Extracting: SignSchool Show-ps1C5M4Q4vg.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 91/99 [1:06:03<1:05:55, 494.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/show/SignSchool Show-ps1C5M4Q4vg.npy\n",
            "\n",
            "‚û° Gloss: sick\n",
            "üé• Extracting: XB3Q6IX6CBo.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sick/XB3Q6IX6CBo.npy\n",
            "üé• Extracting: sick.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sick/sick.npy\n",
            "üé• Extracting: bS9icDOS5g4.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sick/bS9icDOS5g4.npy\n",
            "üé• Extracting: SignSchool Sick-vS3cW9IUUPM.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sick/SignSchool Sick-vS3cW9IUUPM.npy\n",
            "üé• Extracting: 1468766858.7018.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sick/1468766858.7018.npy\n",
            "üé• Extracting: 14644.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sick/14644.npy\n",
            "üé• Extracting: SICK-255.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sick/SICK-255.npy\n",
            "üé• Extracting: 50808.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sick/50808.npy\n",
            "üé• Extracting: NBqpHJiufXU.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sick/NBqpHJiufXU.npy\n",
            "üé• Extracting: oiJ9bPUD1pw.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 92/99 [1:20:52<1:10:31, 604.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sick/oiJ9bPUD1pw.npy\n",
            "\n",
            "‚û° Gloss: sign\n",
            "üé• Extracting: 1532963329.1684.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sign/1532963329.1684.npy\n",
            "üé• Extracting: SignSchool Sign copy-lE1xAUCA2RI.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sign/SignSchool Sign copy-lE1xAUCA2RI.npy\n",
            "üé• Extracting: SignSchool Sign-2d0l-Je3QP4.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sign/SignSchool Sign-2d0l-Je3QP4.npy\n",
            "üé• Extracting: 22309.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sign/22309.npy\n",
            "üé• Extracting: 437018.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sign/437018.npy\n",
            "üé• Extracting: sign.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 93/99 [1:21:23<44:06, 441.09s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sign/sign.npy\n",
            "\n",
            "‚û° Gloss: silly\n",
            "üé• Extracting: 23392.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/silly/23392.npy\n",
            "üé• Extracting: 23532.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/silly/23532.npy\n",
            "üé• Extracting: 9096.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/silly/9096.npy\n",
            "üé• Extracting: SILLY-259.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/silly/SILLY-259.npy\n",
            "üé• Extracting: 94021.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/silly/94021.npy\n",
            "üé• Extracting: silly.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/silly/silly.npy\n",
            "üé• Extracting: SignSchool Silly.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/silly/SignSchool Silly.npy\n",
            "üé• Extracting: 1468767097.7777.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 94/99 [1:22:08<27:12, 326.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/silly/1468767097.7777.npy\n",
            "\n",
            "‚û° Gloss: since\n",
            "üé• Extracting: since.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/since/since.npy\n",
            "üé• Extracting: SINCE-1066.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/since/SINCE-1066.npy\n",
            "üé• Extracting: SINCE-1067.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/since/SINCE-1067.npy\n",
            "üé• Extracting: 116876.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/since/116876.npy\n",
            "üé• Extracting: SignSchool Since-cjBD1unWQJs.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/since/SignSchool Since-cjBD1unWQJs.npy\n",
            "üé• Extracting: 1468767249.6700.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/since/1468767249.6700.npy\n",
            "üé• Extracting: 6309.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 95/99 [1:22:45<16:07, 241.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/since/6309.npy\n",
            "\n",
            "‚û° Gloss: sister\n",
            "üé• Extracting: SignSchool Sister.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sister/SignSchool Sister.npy\n",
            "üé• Extracting: 1468767389.1341.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sister/1468767389.1341.npy\n",
            "üé• Extracting: 7158.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sister/7158.npy\n",
            "üé• Extracting: SISTER-260.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sister/SISTER-260.npy\n",
            "üé• Extracting: sister.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sister/sister.npy\n",
            "üé• Extracting: SignSchool Sister 2-Nir1m7wW0MA.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sister/SignSchool Sister 2-Nir1m7wW0MA.npy\n",
            "üé• Extracting: SignSchool Sister-X7vWbKADW8w.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 96/99 [1:23:19<09:01, 180.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sister/SignSchool Sister-X7vWbKADW8w.npy\n",
            "\n",
            "‚û° Gloss: sit\n",
            "üé• Extracting: sit.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sit/sit.npy\n",
            "üé• Extracting: SignSchool Sat Down-boI0CxQNQrc.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sit/SignSchool Sat Down-boI0CxQNQrc.npy\n",
            "üé• Extracting: SignSchool sit-qpORQ6e0WVY.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sit/SignSchool sit-qpORQ6e0WVY.npy\n",
            "üé• Extracting: 1468767408.4955.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sit/1468767408.4955.npy\n",
            "üé• Extracting: 7822.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 97/99 [1:23:43<04:28, 134.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sit/7822.npy\n",
            "\n",
            "‚û° Gloss: sleep\n",
            "üé• Extracting: sleep.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sleep/sleep.npy\n",
            "üé• Extracting: SignSchool Sleep.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sleep/SignSchool Sleep.npy\n",
            "üé• Extracting: 1468767820.767.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sleep/1468767820.767.npy\n",
            "üé• Extracting: 23907.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing glosses 301-399:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 98/99 [1:24:05<01:40, 100.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/sleep/23907.npy\n",
            "\n",
            "‚û° Gloss: slow\n",
            "üé• Extracting: SLOW-261.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/slow/SLOW-261.npy\n",
            "üé• Extracting: 486598.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/slow/486598.npy\n",
            "üé• Extracting: slow.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/slow/slow.npy\n",
            "üé• Extracting: SignSchool Slow-fUy_ttG5QxY.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/slow/SignSchool Slow-fUy_ttG5QxY.npy\n",
            "üé• Extracting: 1468767957.1850.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/slow/1468767957.1850.npy\n",
            "üé• Extracting: 1522767290.120.mp4\n",
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/slow/1522767290.120.npy\n",
            "üé• Extracting: 14352.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing glosses 301-399: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99/99 [1:24:51<00:00, 51.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saved ‚Üí /content/drive/MyDrive/AFML/Dataset/WLASL_Videos/slow/14352.npy\n",
            "\n",
            "‚úÖ DONE ‚Äî Batch keypoints extracted\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6pEK0KkLRrd",
        "outputId": "94001fff-303a-45f2-b983-04ddba410e06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Stronger CNN + Mamba model + Training Loop for WLASL (498 classes)\n",
        "# Assumes:\n",
        "#   - train_loader and val_loader are already defined\n",
        "#   - Each batch: inputs (B, T, 150), labels (B,)\n",
        "#   - Using Google Drive path for saving best model\n",
        "# ============================================================================\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Mamba Block (simplified, pure PyTorch)\n",
        "# ----------------------------------------------------------------------------\n",
        "class MambaBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Simplified Mamba-style block for temporal modeling.\n",
        "    Works on sequences of shape (B, T, D).\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, d_state: int = 32, expand: int = 2, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_state = d_state\n",
        "        self.d_inner = d_model * expand\n",
        "\n",
        "        # Input projection -> split into (u, gate)\n",
        "        self.in_proj = nn.Linear(d_model, self.d_inner * 2)\n",
        "\n",
        "        # State convolution over time (depthwise)\n",
        "        self.conv = nn.Conv1d(\n",
        "            self.d_inner, self.d_inner,\n",
        "            kernel_size=3, padding=1, groups=self.d_inner\n",
        "        )\n",
        "\n",
        "        self.out_proj = nn.Linear(self.d_inner, d_model)\n",
        "        self.act = nn.SiLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (B, T, D)\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        x = self.norm(x)\n",
        "\n",
        "        # Input projection + gating\n",
        "        u, gate = self.in_proj(x).chunk(2, dim=-1)  # (B, T, d_inner) each\n",
        "        gate = torch.sigmoid(gate)\n",
        "\n",
        "        # Depthwise conv over time\n",
        "        u = u.transpose(1, 2)       # (B, d_inner, T)\n",
        "        u = self.conv(u)            # (B, d_inner, T)\n",
        "        u = u.transpose(1, 2)       # (B, T, d_inner)\n",
        "\n",
        "        u = self.act(u) * gate\n",
        "        u = self.out_proj(u)        # (B, T, D)\n",
        "        u = self.dropout(u)\n",
        "\n",
        "        return residual + u\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Attention Pooling over time\n",
        "# ----------------------------------------------------------------------------\n",
        "class AttentionPool(nn.Module):\n",
        "    def __init__(self, d_model: int):\n",
        "        super().__init__()\n",
        "        self.query = nn.Parameter(torch.randn(d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (B, T, D)\n",
        "        returns: (B, D)\n",
        "        \"\"\"\n",
        "        scores = torch.einsum('btd,d->bt', x, self.query) / math.sqrt(x.size(-1))\n",
        "        weights = F.softmax(scores, dim=1)      # (B, T)\n",
        "        pooled = torch.einsum('btd,bt->bd', x, weights)\n",
        "        return pooled\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Stronger CNN + Mamba model\n",
        "# ----------------------------------------------------------------------------\n",
        "class CNNMambaSignModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int = 150,\n",
        "        num_classes: int = 498,\n",
        "        d_model: int = 256,\n",
        "        num_cnn_layers: int = 2,\n",
        "        num_mamba_layers: int = 2,\n",
        "        dropout: float = 0.2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Project 150-dim keypoints to d_model\n",
        "        self.input_proj = nn.Linear(input_dim, d_model)\n",
        "\n",
        "        # Temporal CNN stack (1D conv over frames)\n",
        "        cnn_layers = []\n",
        "        for _ in range(num_cnn_layers):\n",
        "            cnn_layers.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv1d(d_model, d_model, kernel_size=3, padding=1),\n",
        "                    nn.BatchNorm1d(d_model),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Dropout(dropout),\n",
        "                )\n",
        "            )\n",
        "        self.cnn_layers = nn.ModuleList(cnn_layers)\n",
        "\n",
        "        # Mamba stack for deeper temporal modeling\n",
        "        mamba_layers = []\n",
        "        for _ in range(num_mamba_layers):\n",
        "            mamba_layers.append(\n",
        "                MambaBlock(d_model=d_model, d_state=32, expand=2, dropout=dropout)\n",
        "            )\n",
        "        self.mamba_layers = nn.ModuleList(mamba_layers)\n",
        "\n",
        "        # Temporal attention pooling\n",
        "        self.attn_pool = AttentionPool(d_model)\n",
        "\n",
        "        # Final classifier head\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.LayerNorm(d_model),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (B, T, 150)\n",
        "        \"\"\"\n",
        "        # Project to d_model\n",
        "        x = self.input_proj(x)          # (B, T, D)\n",
        "\n",
        "        # CNN expects (B, D, T)\n",
        "        x = x.transpose(1, 2)           # (B, D, T)\n",
        "        for layer in self.cnn_layers:\n",
        "            residual = x\n",
        "            out = layer(x)\n",
        "            x = out + residual          # residual CNN\n",
        "\n",
        "        # Back to (B, T, D)\n",
        "        x = x.transpose(1, 2)           # (B, T, D)\n",
        "\n",
        "        # Mamba layers\n",
        "        for layer in self.mamba_layers:\n",
        "            x = layer(x)                # (B, T, D)\n",
        "\n",
        "        # Attention pooling over time\n",
        "        x = self.attn_pool(x)           # (B, D)\n",
        "\n",
        "        # Classifier\n",
        "        logits = self.fc(x)             # (B, num_classes)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Accuracy helpers (Top-1, Top-5)\n",
        "# ----------------------------------------------------------------------------\n",
        "def accuracy_topk(logits, targets, topk=(1, 5)):\n",
        "    \"\"\"\n",
        "    logits: (B, C)\n",
        "    targets: (B,)\n",
        "    Returns: list of accuracies for each k in topk\n",
        "    \"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = targets.size(0)\n",
        "\n",
        "    _, pred = logits.topk(maxk, dim=1, largest=True, sorted=True)  # (B, maxk)\n",
        "    pred = pred.t()                                                # (maxk, B)\n",
        "    correct = pred.eq(targets.view(1, -1).expand_as(pred))        # (maxk, B)\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
        "        res.append((correct_k * (100.0 / batch_size)).item())\n",
        "    return res  # [top1, top5, ...]\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Instantiate model, loss, optimizer, scheduler\n",
        "# ----------------------------------------------------------------------------\n",
        "num_classes = 498\n",
        "input_dim = 150\n",
        "num_epochs = 60\n",
        "\n",
        "model = CNNMambaSignModel(\n",
        "    input_dim=input_dim,\n",
        "    num_classes=num_classes,\n",
        "    d_model=256,\n",
        "    num_cnn_layers=2,\n",
        "    num_mamba_layers=2,\n",
        "    dropout=0.2,\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        ")\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer, T_max=num_epochs\n",
        ")\n",
        "\n",
        "print(model)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Training & Validation Loop\n",
        "# ----------------------------------------------------------------------------\n",
        "best_val_top1 = 0.0\n",
        "save_path = \"/content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\"\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # ------------------------\n",
        "    # Train\n",
        "    # ------------------------\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_top1 = 0.0\n",
        "    running_top5 = 0.0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device)      # (B, T, 150)\n",
        "        labels = labels.to(device)      # (B,)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(inputs)          # (B, num_classes)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_size = labels.size(0)\n",
        "        total_train += batch_size\n",
        "        running_loss += loss.item() * batch_size\n",
        "\n",
        "        top1, top5 = accuracy_topk(logits, labels, topk=(1, 5))\n",
        "        running_top1 += top1 * batch_size / 100.0\n",
        "        running_top5 += top5 * batch_size / 100.0\n",
        "\n",
        "    train_loss = running_loss / total_train\n",
        "    train_top1 = (running_top1 / total_train) * 100.0\n",
        "    train_top5 = (running_top5 / total_train) * 100.0\n",
        "\n",
        "    # ------------------------\n",
        "    # Validation\n",
        "    # ------------------------\n",
        "    model.eval()\n",
        "    val_loss_sum = 0.0\n",
        "    val_top1_sum = 0.0\n",
        "    val_top5_sum = 0.0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            batch_size = labels.size(0)\n",
        "            total_val += batch_size\n",
        "            val_loss_sum += loss.item() * batch_size\n",
        "\n",
        "            top1, top5 = accuracy_topk(logits, labels, topk=(1, 5))\n",
        "            val_top1_sum += top1 * batch_size / 100.0\n",
        "            val_top5_sum += top5 * batch_size / 100.0\n",
        "\n",
        "    val_loss = val_loss_sum / total_val\n",
        "    val_top1 = (val_top1_sum / total_val) * 100.0\n",
        "    val_top5 = (val_top5_sum / total_val) * 100.0\n",
        "\n",
        "    # Step LR scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
        "    print(f\"Train loss: {train_loss:.4f} | Top1: {train_top1:.2f}% | Top5: {train_top5:.2f}%\")\n",
        "    print(f\"Val   loss: {val_loss:.4f} | Top1: {val_top1:.2f}% | Top5: {val_top5:.2f}%\")\n",
        "\n",
        "    # Save best model by val_top1\n",
        "    if val_top1 > best_val_top1:\n",
        "        best_val_top1 = val_top1\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"‚úÖ New best model saved with val_top1={best_val_top1:.2f}% ‚Üí {save_path}\")\n",
        "    else:\n",
        "        print(f\"No improvement. Best val_top1 so far: {best_val_top1:.2f}%\")\n",
        "\n",
        "print(\"\\nTraining finished.\")\n",
        "print(f\"Best validation Top-1 accuracy: {best_val_top1:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQHbq1yrXvUT",
        "outputId": "b0a57322-8d90-419f-c398-746b54e610f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "CNNMambaSignModel(\n",
            "  (input_proj): Linear(in_features=150, out_features=256, bias=True)\n",
            "  (cnn_layers): ModuleList(\n",
            "    (0-1): 2 x Sequential(\n",
            "      (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): SiLU()\n",
            "      (3): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (mamba_layers): ModuleList(\n",
            "    (0-1): 2 x MambaBlock(\n",
            "      (in_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
            "      (conv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512)\n",
            "      (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "      (act): SiLU()\n",
            "      (dropout): Dropout(p=0.2, inplace=False)\n",
            "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (attn_pool): AttentionPool()\n",
            "  (fc): Sequential(\n",
            "    (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (1): Dropout(p=0.2, inplace=False)\n",
            "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (3): SiLU()\n",
            "    (4): Dropout(p=0.2, inplace=False)\n",
            "    (5): Linear(in_features=256, out_features=498, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Epoch 1/60\n",
            "Train loss: 6.2468 | Top1: 0.44% | Top5: 1.35%\n",
            "Val   loss: 6.2430 | Top1: 0.79% | Top5: 0.98%\n",
            "‚úÖ New best model saved with val_top1=0.79% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 2/60\n",
            "Train loss: 6.0211 | Top1: 1.01% | Top5: 4.35%\n",
            "Val   loss: 5.9938 | Top1: 1.18% | Top5: 3.54%\n",
            "‚úÖ New best model saved with val_top1=1.18% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 3/60\n",
            "Train loss: 5.8064 | Top1: 1.04% | Top5: 4.31%\n",
            "Val   loss: 5.8710 | Top1: 1.18% | Top5: 3.93%\n",
            "No improvement. Best val_top1 so far: 1.18%\n",
            "\n",
            "Epoch 4/60\n",
            "Train loss: 5.7119 | Top1: 1.75% | Top5: 6.27%\n",
            "Val   loss: 5.7713 | Top1: 1.38% | Top5: 4.52%\n",
            "‚úÖ New best model saved with val_top1=1.38% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 5/60\n",
            "Train loss: 5.6002 | Top1: 1.75% | Top5: 8.02%\n",
            "Val   loss: 5.6602 | Top1: 1.57% | Top5: 4.91%\n",
            "‚úÖ New best model saved with val_top1=1.57% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 6/60\n",
            "Train loss: 5.4874 | Top1: 2.66% | Top5: 9.57%\n",
            "Val   loss: 5.5582 | Top1: 1.18% | Top5: 7.86%\n",
            "No improvement. Best val_top1 so far: 1.57%\n",
            "\n",
            "Epoch 7/60\n",
            "Train loss: 5.4137 | Top1: 2.86% | Top5: 10.95%\n",
            "Val   loss: 5.4971 | Top1: 2.95% | Top5: 9.43%\n",
            "‚úÖ New best model saved with val_top1=2.95% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 8/60\n",
            "Train loss: 5.3054 | Top1: 3.51% | Top5: 12.54%\n",
            "Val   loss: 5.6254 | Top1: 2.55% | Top5: 7.86%\n",
            "No improvement. Best val_top1 so far: 2.95%\n",
            "\n",
            "Epoch 9/60\n",
            "Train loss: 5.1914 | Top1: 3.94% | Top5: 15.87%\n",
            "Val   loss: 5.3683 | Top1: 2.36% | Top5: 11.59%\n",
            "No improvement. Best val_top1 so far: 2.95%\n",
            "\n",
            "Epoch 10/60\n",
            "Train loss: 5.1077 | Top1: 4.79% | Top5: 17.73%\n",
            "Val   loss: 5.3508 | Top1: 3.73% | Top5: 11.98%\n",
            "‚úÖ New best model saved with val_top1=3.73% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 11/60\n",
            "Train loss: 5.0229 | Top1: 5.19% | Top5: 18.67%\n",
            "Val   loss: 5.2356 | Top1: 5.50% | Top5: 15.52%\n",
            "‚úÖ New best model saved with val_top1=5.50% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 12/60\n",
            "Train loss: 4.9353 | Top1: 5.73% | Top5: 20.86%\n",
            "Val   loss: 5.2266 | Top1: 3.73% | Top5: 17.49%\n",
            "No improvement. Best val_top1 so far: 5.50%\n",
            "\n",
            "Epoch 13/60\n",
            "Train loss: 4.8708 | Top1: 6.03% | Top5: 23.02%\n",
            "Val   loss: 5.2786 | Top1: 3.34% | Top5: 16.70%\n",
            "No improvement. Best val_top1 so far: 5.50%\n",
            "\n",
            "Epoch 14/60\n",
            "Train loss: 4.7632 | Top1: 6.84% | Top5: 23.80%\n",
            "Val   loss: 5.1408 | Top1: 5.11% | Top5: 18.07%\n",
            "No improvement. Best val_top1 so far: 5.50%\n",
            "\n",
            "Epoch 15/60\n",
            "Train loss: 4.7122 | Top1: 7.15% | Top5: 24.37%\n",
            "Val   loss: 5.1600 | Top1: 5.89% | Top5: 16.90%\n",
            "‚úÖ New best model saved with val_top1=5.89% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 16/60\n",
            "Train loss: 4.6481 | Top1: 8.83% | Top5: 26.29%\n",
            "Val   loss: 5.2678 | Top1: 3.54% | Top5: 14.34%\n",
            "No improvement. Best val_top1 so far: 5.89%\n",
            "\n",
            "Epoch 17/60\n",
            "Train loss: 4.5968 | Top1: 7.75% | Top5: 26.69%\n",
            "Val   loss: 5.0321 | Top1: 6.68% | Top5: 20.63%\n",
            "‚úÖ New best model saved with val_top1=6.68% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 18/60\n",
            "Train loss: 4.4946 | Top1: 9.30% | Top5: 30.20%\n",
            "Val   loss: 5.0763 | Top1: 5.50% | Top5: 18.47%\n",
            "No improvement. Best val_top1 so far: 6.68%\n",
            "\n",
            "Epoch 19/60\n",
            "Train loss: 4.4455 | Top1: 10.62% | Top5: 31.75%\n",
            "Val   loss: 5.1373 | Top1: 5.30% | Top5: 17.29%\n",
            "No improvement. Best val_top1 so far: 6.68%\n",
            "\n",
            "Epoch 20/60\n",
            "Train loss: 4.3702 | Top1: 11.16% | Top5: 33.84%\n",
            "Val   loss: 5.0518 | Top1: 5.11% | Top5: 20.43%\n",
            "No improvement. Best val_top1 so far: 6.68%\n",
            "\n",
            "Epoch 21/60\n",
            "Train loss: 4.3011 | Top1: 11.66% | Top5: 36.23%\n",
            "Val   loss: 4.9552 | Top1: 9.23% | Top5: 25.54%\n",
            "‚úÖ New best model saved with val_top1=9.23% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 22/60\n",
            "Train loss: 4.2155 | Top1: 12.84% | Top5: 38.32%\n",
            "Val   loss: 5.0087 | Top1: 8.84% | Top5: 23.77%\n",
            "No improvement. Best val_top1 so far: 9.23%\n",
            "\n",
            "Epoch 23/60\n",
            "Train loss: 4.1608 | Top1: 14.39% | Top5: 40.14%\n",
            "Val   loss: 4.9642 | Top1: 7.86% | Top5: 26.13%\n",
            "No improvement. Best val_top1 so far: 9.23%\n",
            "\n",
            "Epoch 24/60\n",
            "Train loss: 4.0851 | Top1: 14.69% | Top5: 42.53%\n",
            "Val   loss: 4.9777 | Top1: 7.27% | Top5: 26.13%\n",
            "No improvement. Best val_top1 so far: 9.23%\n",
            "\n",
            "Epoch 25/60\n",
            "Train loss: 4.0304 | Top1: 16.51% | Top5: 44.02%\n",
            "Val   loss: 4.9369 | Top1: 9.43% | Top5: 27.31%\n",
            "‚úÖ New best model saved with val_top1=9.43% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 26/60\n",
            "Train loss: 3.9803 | Top1: 17.29% | Top5: 46.38%\n",
            "Val   loss: 4.9561 | Top1: 9.23% | Top5: 28.29%\n",
            "No improvement. Best val_top1 so far: 9.43%\n",
            "\n",
            "Epoch 27/60\n",
            "Train loss: 3.9045 | Top1: 19.31% | Top5: 48.23%\n",
            "Val   loss: 4.9327 | Top1: 10.61% | Top5: 30.26%\n",
            "‚úÖ New best model saved with val_top1=10.61% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 28/60\n",
            "Train loss: 3.8535 | Top1: 20.05% | Top5: 49.92%\n",
            "Val   loss: 4.9351 | Top1: 8.45% | Top5: 28.68%\n",
            "No improvement. Best val_top1 so far: 10.61%\n",
            "\n",
            "Epoch 29/60\n",
            "Train loss: 3.7855 | Top1: 22.28% | Top5: 52.78%\n",
            "Val   loss: 4.9670 | Top1: 9.04% | Top5: 31.24%\n",
            "No improvement. Best val_top1 so far: 10.61%\n",
            "\n",
            "Epoch 30/60\n",
            "Train loss: 3.7282 | Top1: 22.99% | Top5: 54.97%\n",
            "Val   loss: 4.9322 | Top1: 9.82% | Top5: 30.26%\n",
            "No improvement. Best val_top1 so far: 10.61%\n",
            "\n",
            "Epoch 31/60\n",
            "Train loss: 3.6678 | Top1: 23.05% | Top5: 56.05%\n",
            "Val   loss: 4.8491 | Top1: 10.81% | Top5: 30.26%\n",
            "‚úÖ New best model saved with val_top1=10.81% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 32/60\n",
            "Train loss: 3.6217 | Top1: 24.17% | Top5: 58.51%\n",
            "Val   loss: 4.9789 | Top1: 9.04% | Top5: 30.45%\n",
            "No improvement. Best val_top1 so far: 10.81%\n",
            "\n",
            "Epoch 33/60\n",
            "Train loss: 3.5531 | Top1: 26.96% | Top5: 60.23%\n",
            "Val   loss: 4.9069 | Top1: 10.41% | Top5: 31.24%\n",
            "No improvement. Best val_top1 so far: 10.81%\n",
            "\n",
            "Epoch 34/60\n",
            "Train loss: 3.5102 | Top1: 28.31% | Top5: 61.78%\n",
            "Val   loss: 4.9266 | Top1: 12.18% | Top5: 32.42%\n",
            "‚úÖ New best model saved with val_top1=12.18% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 35/60\n",
            "Train loss: 3.4701 | Top1: 29.32% | Top5: 61.78%\n",
            "Val   loss: 4.9106 | Top1: 9.63% | Top5: 31.83%\n",
            "No improvement. Best val_top1 so far: 12.18%\n",
            "\n",
            "Epoch 36/60\n",
            "Train loss: 3.4230 | Top1: 29.96% | Top5: 64.04%\n",
            "Val   loss: 4.9503 | Top1: 11.20% | Top5: 32.42%\n",
            "No improvement. Best val_top1 so far: 12.18%\n",
            "\n",
            "Epoch 37/60\n",
            "Train loss: 3.3692 | Top1: 31.92% | Top5: 66.90%\n",
            "Val   loss: 4.9738 | Top1: 11.39% | Top5: 33.60%\n",
            "No improvement. Best val_top1 so far: 12.18%\n",
            "\n",
            "Epoch 38/60\n",
            "Train loss: 3.3469 | Top1: 32.02% | Top5: 66.87%\n",
            "Val   loss: 4.9575 | Top1: 10.22% | Top5: 32.61%\n",
            "No improvement. Best val_top1 so far: 12.18%\n",
            "\n",
            "Epoch 39/60\n",
            "Train loss: 3.2735 | Top1: 33.91% | Top5: 68.52%\n",
            "Val   loss: 5.0498 | Top1: 11.00% | Top5: 31.43%\n",
            "No improvement. Best val_top1 so far: 12.18%\n",
            "\n",
            "Epoch 40/60\n",
            "Train loss: 3.2776 | Top1: 34.51% | Top5: 69.06%\n",
            "Val   loss: 5.0146 | Top1: 11.59% | Top5: 32.61%\n",
            "No improvement. Best val_top1 so far: 12.18%\n",
            "\n",
            "Epoch 41/60\n",
            "Train loss: 3.2254 | Top1: 36.13% | Top5: 70.68%\n",
            "Val   loss: 5.0200 | Top1: 11.39% | Top5: 34.58%\n",
            "No improvement. Best val_top1 so far: 12.18%\n",
            "\n",
            "Epoch 42/60\n",
            "Train loss: 3.1816 | Top1: 36.80% | Top5: 72.13%\n",
            "Val   loss: 5.0174 | Top1: 11.00% | Top5: 34.58%\n",
            "No improvement. Best val_top1 so far: 12.18%\n",
            "\n",
            "Epoch 43/60\n",
            "Train loss: 3.1374 | Top1: 37.78% | Top5: 73.78%\n",
            "Val   loss: 4.9621 | Top1: 12.18% | Top5: 33.99%\n",
            "‚úÖ New best model saved with val_top1=12.18% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 44/60\n",
            "Train loss: 3.1123 | Top1: 39.30% | Top5: 73.91%\n",
            "Val   loss: 5.0380 | Top1: 12.18% | Top5: 33.60%\n",
            "No improvement. Best val_top1 so far: 12.18%\n",
            "\n",
            "Epoch 45/60\n",
            "Train loss: 3.0980 | Top1: 40.21% | Top5: 74.69%\n",
            "Val   loss: 5.0422 | Top1: 11.59% | Top5: 32.61%\n",
            "No improvement. Best val_top1 so far: 12.18%\n",
            "\n",
            "Epoch 46/60\n",
            "Train loss: 3.0405 | Top1: 41.96% | Top5: 76.31%\n",
            "Val   loss: 5.0392 | Top1: 11.98% | Top5: 33.20%\n",
            "No improvement. Best val_top1 so far: 12.18%\n",
            "\n",
            "Epoch 47/60\n",
            "Train loss: 3.0363 | Top1: 41.79% | Top5: 75.50%\n",
            "Val   loss: 5.0243 | Top1: 12.57% | Top5: 33.60%\n",
            "‚úÖ New best model saved with val_top1=12.57% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 48/60\n",
            "Train loss: 3.0139 | Top1: 42.26% | Top5: 76.58%\n",
            "Val   loss: 5.0068 | Top1: 12.97% | Top5: 33.99%\n",
            "‚úÖ New best model saved with val_top1=12.97% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 49/60\n",
            "Train loss: 2.9974 | Top1: 43.31% | Top5: 77.18%\n",
            "Val   loss: 5.0453 | Top1: 12.18% | Top5: 34.77%\n",
            "No improvement. Best val_top1 so far: 12.97%\n",
            "\n",
            "Epoch 50/60\n",
            "Train loss: 2.9634 | Top1: 44.08% | Top5: 78.33%\n",
            "Val   loss: 5.0436 | Top1: 12.77% | Top5: 34.38%\n",
            "No improvement. Best val_top1 so far: 12.97%\n",
            "\n",
            "Epoch 51/60\n",
            "Train loss: 2.9427 | Top1: 45.30% | Top5: 78.50%\n",
            "Val   loss: 5.0507 | Top1: 12.77% | Top5: 34.58%\n",
            "No improvement. Best val_top1 so far: 12.97%\n",
            "\n",
            "Epoch 52/60\n",
            "Train loss: 2.9365 | Top1: 45.64% | Top5: 78.36%\n",
            "Val   loss: 5.0415 | Top1: 12.38% | Top5: 34.38%\n",
            "No improvement. Best val_top1 so far: 12.97%\n",
            "\n",
            "Epoch 53/60\n",
            "Train loss: 2.9186 | Top1: 45.70% | Top5: 78.60%\n",
            "Val   loss: 5.0571 | Top1: 12.18% | Top5: 34.18%\n",
            "No improvement. Best val_top1 so far: 12.97%\n",
            "\n",
            "Epoch 54/60\n",
            "Train loss: 2.9047 | Top1: 46.21% | Top5: 79.47%\n",
            "Val   loss: 5.0699 | Top1: 12.57% | Top5: 34.58%\n",
            "No improvement. Best val_top1 so far: 12.97%\n",
            "\n",
            "Epoch 55/60\n",
            "Train loss: 2.9167 | Top1: 46.58% | Top5: 78.77%\n",
            "Val   loss: 5.0458 | Top1: 13.16% | Top5: 34.58%\n",
            "‚úÖ New best model saved with val_top1=13.16% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 56/60\n",
            "Train loss: 2.9177 | Top1: 45.70% | Top5: 79.71%\n",
            "Val   loss: 5.0665 | Top1: 12.77% | Top5: 34.38%\n",
            "No improvement. Best val_top1 so far: 13.16%\n",
            "\n",
            "Epoch 57/60\n",
            "Train loss: 2.9265 | Top1: 45.47% | Top5: 79.10%\n",
            "Val   loss: 5.0567 | Top1: 12.77% | Top5: 34.97%\n",
            "No improvement. Best val_top1 so far: 13.16%\n",
            "\n",
            "Epoch 58/60\n",
            "Train loss: 2.9094 | Top1: 45.40% | Top5: 78.70%\n",
            "Val   loss: 5.0535 | Top1: 12.77% | Top5: 35.36%\n",
            "No improvement. Best val_top1 so far: 13.16%\n",
            "\n",
            "Epoch 59/60\n",
            "Train loss: 2.8914 | Top1: 46.75% | Top5: 79.84%\n",
            "Val   loss: 5.0565 | Top1: 12.77% | Top5: 34.77%\n",
            "No improvement. Best val_top1 so far: 13.16%\n",
            "\n",
            "Epoch 60/60\n",
            "Train loss: 2.8988 | Top1: 46.44% | Top5: 80.22%\n",
            "Val   loss: 5.0748 | Top1: 12.77% | Top5: 34.38%\n",
            "No improvement. Best val_top1 so far: 13.16%\n",
            "\n",
            "Training finished.\n",
            "Best validation Top-1 accuracy: 13.16%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# -----------------------------\n",
        "# CONFIG\n",
        "# -----------------------------\n",
        "DATASET_ROOT = \"/content/drive/MyDrive/AFML/Dataset/WLASL_Videos\"\n",
        "\n",
        "# If your .npy files are inside a \"preprocessed\" subfolder, change this:\n",
        "# PREPROCESSED_ROOT = os.path.join(DATASET_ROOT, \"preprocessed\")\n",
        "PREPROCESSED_ROOT = DATASET_ROOT   # adjust if needed\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 60\n",
        "SEQ_LEN = 64          # <<< fixed number of frames per sample\n",
        "SAVE_PATH = \"/content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\"\n",
        "\n",
        "# New hyperparameters\n",
        "D_MODEL           = 192\n",
        "NUM_CNN_LAYERS    = 2\n",
        "NUM_MAMBA_LAYERS  = 2\n",
        "DROPOUT_RATE      = 0.3\n",
        "LR                = 5e-4\n",
        "WEIGHT_DECAY      = 5e-4\n",
        "LABEL_SMOOTHING   = 0.1\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ============================================================================\n",
        "# 1. DATASET + DATALOADER (with pad/crop to SEQ_LEN)\n",
        "# ============================================================================\n",
        "\n",
        "class WLASLKeypointDataset(Dataset):\n",
        "    def __init__(self, root: str, samples: List[Tuple[str, int]], seq_len: int = 64):\n",
        "        \"\"\"\n",
        "        root: base folder where .npy files exist\n",
        "        samples: list of (relative_path, label)\n",
        "        seq_len: fixed number of frames (T) after pad/crop\n",
        "        \"\"\"\n",
        "        self.root = root\n",
        "        self.samples = samples\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        rel_path, label = self.samples[idx]\n",
        "        full_path = os.path.join(self.root, rel_path)\n",
        "\n",
        "        keypoints = np.load(full_path)   # original shape: (T, 150)\n",
        "        T, D = keypoints.shape\n",
        "\n",
        "        # --- Pad or crop to self.seq_len ---\n",
        "        if T > self.seq_len:\n",
        "            # center crop\n",
        "            start = (T - self.seq_len) // 2\n",
        "            keypoints = keypoints[start:start + self.seq_len]\n",
        "        elif T < self.seq_len:\n",
        "            pad_len = self.seq_len - T\n",
        "            pad = np.zeros((pad_len, D), dtype=keypoints.dtype)\n",
        "            keypoints = np.concatenate([keypoints, pad], axis=0)\n",
        "        # now shape = (seq_len, 150)\n",
        "\n",
        "        keypoints = torch.tensor(keypoints, dtype=torch.float32)\n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return keypoints, label\n",
        "\n",
        "\n",
        "# Build file list + label map\n",
        "glosses = sorted([\n",
        "    d for d in os.listdir(PREPROCESSED_ROOT)\n",
        "    if os.path.isdir(os.path.join(PREPROCESSED_ROOT, d))\n",
        "])\n",
        "\n",
        "label_map = {gloss: i for i, gloss in enumerate(glosses)}\n",
        "\n",
        "all_files = []\n",
        "missing_count = 0\n",
        "\n",
        "print(\"Scanning gloss folders and checking .npy files...\")\n",
        "for gloss in tqdm(glosses):\n",
        "    gloss_folder = os.path.join(PREPROCESSED_ROOT, gloss)\n",
        "    for f in os.listdir(gloss_folder):\n",
        "        if not f.endswith(\".npy\"):\n",
        "            continue\n",
        "        rel_path = f\"{gloss}/{f}\"\n",
        "        full_path = os.path.join(PREPROCESSED_ROOT, rel_path)\n",
        "        if os.path.isfile(full_path):\n",
        "            label = label_map[gloss]\n",
        "            all_files.append((rel_path, label))\n",
        "        else:\n",
        "            missing_count += 1\n",
        "\n",
        "print(f\"\\nNum valid samples total: {len(all_files)}\")\n",
        "print(f\"Num missing/invalid files skipped: {missing_count}\")\n",
        "\n",
        "# Filter out classes with only one sample before splitting\n",
        "labels_for_split = [lbl for _, lbl in all_files]\n",
        "label_counts = Counter(labels_for_split)\n",
        "single_sample_labels = {label for label, count in label_counts.items() if count < 2}\n",
        "\n",
        "filtered_all_files = [\n",
        "    (rel_path, label) for rel_path, label in all_files\n",
        "    if label not in single_sample_labels\n",
        "]\n",
        "\n",
        "# Identify which of the original glosses are still present\n",
        "unique_original_labels_in_filtered = sorted(list(set([label for _, label in filtered_all_files])))\n",
        "present_gloss_names = [glosses[lbl] for lbl in unique_original_labels_in_filtered]\n",
        "new_label_map = {gloss_name: i for i, gloss_name in enumerate(present_gloss_names)}\n",
        "\n",
        "# Update labels in filtered_all_files to use the new contiguous labels\n",
        "final_all_files = [\n",
        "    (rel_path, new_label_map[glosses[old_label]]) for rel_path, old_label in filtered_all_files\n",
        "]\n",
        "\n",
        "num_classes = len(present_gloss_names)\n",
        "print(f\"Num classes (after filtering single-sample): {num_classes}\")\n",
        "print(f\"Num samples after filtering: {len(final_all_files)}\")\n",
        "\n",
        "# Train/val split (stratified) with filtered data\n",
        "labels_for_split = [lbl for _, lbl in final_all_files]\n",
        "train_samples, val_samples = train_test_split(\n",
        "    final_all_files,\n",
        "    test_size=0.15,\n",
        "    random_state=42,\n",
        "    stratify=labels_for_split\n",
        ")\n",
        "\n",
        "print(f\"Num train samples: {len(train_samples)}\")\n",
        "print(f\"Num val samples  : {len(val_samples)}\")\n",
        "\n",
        "train_dataset = WLASLKeypointDataset(PREPROCESSED_ROOT, train_samples, seq_len=SEQ_LEN)\n",
        "val_dataset   = WLASLKeypointDataset(PREPROCESSED_ROOT, val_samples,   seq_len=SEQ_LEN)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "# Quick sanity check\n",
        "batch = next(iter(train_loader))\n",
        "print(\"Batch keypoints shape:\", batch[0].shape)  # (B, 64, 150)\n",
        "print(\"Batch labels shape   :\", batch[1].shape)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 2. MODEL: CNN + Mamba-style temporal block\n",
        "# ============================================================================\n",
        "\n",
        "class MambaBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Simplified Mamba-style block for temporal modeling.\n",
        "    Input:  (B, T, D)\n",
        "    Output: (B, T, D)\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, d_state: int = 32, expand: int = 2, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_state = d_state\n",
        "        self.d_inner = d_model * expand * 2  # 2x for (u, gate)\n",
        "\n",
        "        # Input projection ‚Üí (u, gate)\n",
        "        self.in_proj = nn.Linear(d_model, self.d_inner)\n",
        "\n",
        "        # Depthwise temporal conv on u\n",
        "        self.conv = nn.Conv1d(\n",
        "            self.d_inner // 2,\n",
        "            self.d_inner // 2,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "            groups=self.d_inner // 2,\n",
        "        )\n",
        "\n",
        "        self.out_proj = nn.Linear(self.d_inner // 2, d_model)\n",
        "        self.act = nn.SiLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (B, T, D)\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        x = self.norm(x)\n",
        "\n",
        "        h = self.in_proj(x)                   # (B, T, d_inner)\n",
        "        u, gate = h.chunk(2, dim=-1)          # (B, T, d_inner/2) each\n",
        "        gate = torch.sigmoid(gate)\n",
        "\n",
        "        # depthwise conv over time\n",
        "        u = u.transpose(1, 2)                 # (B, d_inner/2, T)\n",
        "        u = self.conv(u)                      # (B, d_inner/2, T)\n",
        "        u = u.transpose(1, 2)                 # (B, T, d_inner/2)\n",
        "\n",
        "        u = self.act(u) * gate\n",
        "        u = self.out_proj(u)                  # (B, T, D)\n",
        "        u = self.dropout(u)\n",
        "\n",
        "        return residual + u\n",
        "\n",
        "\n",
        "class AttentionPool(nn.Module):\n",
        "    def __init__(self, d_model: int):\n",
        "        super().__init__()\n",
        "        self.query = nn.Parameter(torch.randn(d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (B, T, D)\n",
        "        returns: (B, D)\n",
        "        \"\"\"\n",
        "        scores = torch.einsum(\"btd,d->bt\", x, self.query) / math.sqrt(x.size(-1))\n",
        "        weights = F.softmax(scores, dim=1)          # (B, T)\n",
        "        pooled = torch.einsum(\"btd,bt->bd\", x, weights)\n",
        "        return pooled\n",
        "\n",
        "\n",
        "class CNNMambaSignModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int = 150,\n",
        "        num_classes: int = 498,\n",
        "        d_model: int = 256,\n",
        "        num_cnn_layers: int = 2,\n",
        "        num_mamba_layers: int = 2,\n",
        "        dropout: float = 0.2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Project raw keypoints (150) to model dim (256)\n",
        "        self.input_proj = nn.Linear(input_dim, d_model)\n",
        "\n",
        "        # Temporal CNN stack: Conv1d over time\n",
        "        cnn_layers = []\n",
        "        for _ in range(num_cnn_layers):\n",
        "            cnn_layers.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv1d(d_model, d_model, kernel_size=3, padding=1),\n",
        "                    nn.BatchNorm1d(d_model),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Dropout(dropout),\n",
        "                )\n",
        "            )\n",
        "        self.cnn_layers = nn.ModuleList(cnn_layers)\n",
        "\n",
        "        # Mamba-style temporal stack\n",
        "        mamba_layers = []\n",
        "        for _ in range(num_mamba_layers):\n",
        "            mamba_layers.append(\n",
        "                MambaBlock(d_model=d_model, d_state=32, expand=2, dropout=dropout)\n",
        "            )\n",
        "        self.mamba_layers = nn.ModuleList(mamba_layers)\n",
        "\n",
        "        # Attention pooling over time\n",
        "        self.attn_pool = AttentionPool(d_model)\n",
        "\n",
        "        # Classifier head\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.LayerNorm(d_model),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (B, T, 150)\n",
        "        \"\"\"\n",
        "        x = self.input_proj(x)          # (B, T, D)\n",
        "\n",
        "        # CNN expects (B, D, T)\n",
        "        x = x.transpose(1, 2)           # (B, D, T)\n",
        "        for layer in self.cnn_layers:\n",
        "            residual = x\n",
        "            out = layer(x)\n",
        "            x = out + residual          # residual connection\n",
        "\n",
        "        # Back to (B, T, D)\n",
        "        x = x.transpose(1, 2)           # (B, T, D)\n",
        "\n",
        "        # Mamba layers\n",
        "        for layer in self.mamba_layers:\n",
        "            x = layer(x)                # (B, T, D)\n",
        "\n",
        "        # Attention pool over time\n",
        "        x = self.attn_pool(x)           # (B, D)\n",
        "\n",
        "        # Classifier\n",
        "        logits = self.fc(x)             # (B, num_classes)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 3. TRAINING SETUP\n",
        "# ============================================================================\n",
        "\n",
        "def accuracy_topk(logits, targets, topk=(1, 5)):\n",
        "    \"\"\"\n",
        "    logits: (B, C)\n",
        "    targets: (B,)\n",
        "    returns: [top1, top5, ...] in %\n",
        "    \"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = targets.size(0)\n",
        "\n",
        "    _, pred = logits.topk(maxk, dim=1, largest=True, sorted=True)  # (B, maxk)\n",
        "    pred = pred.t()                                                # (maxk, B)\n",
        "    correct = pred.eq(targets.view(1, -1).expand_as(pred))        # (maxk, B)\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
        "        res.append((correct_k * (100.0 / batch_size)).item()))\n",
        "    return res\n",
        "\n",
        "\n",
        "model = CNNMambaSignModel(\n",
        "    input_dim=150,\n",
        "    num_classes=num_classes,\n",
        "    d_model=D_MODEL,\n",
        "    num_cnn_layers=NUM_CNN_LAYERS,\n",
        "    num_mamba_layers=NUM_MAMBA_LAYERS,\n",
        "    dropout=DROPOUT_RATE,\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=LR,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        ")\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer, T_max=NUM_EPOCHS\n",
        ")\n",
        "\n",
        "print(model)\n",
        "\n",
        "# ============================================================================\n",
        "# 4. TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "best_val_top1 = 0.0\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    # ---------------- Train ----------------\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_top1 = 0.0\n",
        "    running_top5 = 0.0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device)      # (B, 64, 150)\n",
        "        labels = labels.to(device)      # (B,)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(inputs)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_size = labels.size(0)\n",
        "        total_train += batch_size\n",
        "        running_loss += loss.item() * batch_size\n",
        "\n",
        "        top1, top5 = accuracy_topk(logits, labels, topk=(1, 5))\n",
        "        running_top1 += top1 * batch_size / 100.0\n",
        "        running_top5 += top5 * batch_size / 100.0\n",
        "\n",
        "    train_loss = running_loss / total_train\n",
        "    train_top1 = (running_top1 / total_train) * 100.0\n",
        "    train_top5 = (running_top5 / total_train) * 100.0\n",
        "\n",
        "    # ---------------- Val ----------------\n",
        "    model.eval()\n",
        "    val_loss_sum = 0.0\n",
        "    val_top1_sum = 0.0\n",
        "    val_top5_sum = 0.0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            batch_size = labels.size(0)\n",
        "            total_val += batch_size\n",
        "            val_loss_sum += loss.item() * batch_size\n",
        "\n",
        "            top1, top5 = accuracy_topk(logits, labels, topk=(1, 5))\n",
        "            val_top1_sum += top1 * batch_size / 100.0\n",
        "            val_top5_sum += top5 * batch_size / 100.0\n",
        "\n",
        "    val_loss = val_loss_sum / total_val\n",
        "    val_top1 = (val_top1_sum / total_val) * 100.0\n",
        "    val_top5 = (val_top5_sum / total_val) * 100.0\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"\\nEpoch {epoch}/{NUM_EPOCHS}\")\n",
        "    print(f\"Train loss: {train_loss:.4f} | Top1: {train_top1:.2f}% | Top5: {train_top5:.2f}%\")\n",
        "    print(f\"Val   loss: {val_loss:.4f} | Top1: {val_top1:.2f}% | Top5: {val_top5:.2f}%\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_top1 > best_val_top1:\n",
        "        best_val_top1 = val_top1\n",
        "        torch.save(model.state_dict(), SAVE_PATH)\n",
        "        print(f\"\\u2705 New best model saved with val_top1={best_val_top1:.2f}% \\u2192 {SAVE_PATH}\")\n",
        "    else:\n",
        "        print(f\"No improvement. Best val_top1 so far: {best_val_top1:.2f}%\")\n",
        "\n",
        "print(\"\\nTraining finished.\")\n",
        "print(f\"Best validation Top-1 accuracy: {best_val_top1:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyOaNdS045YO",
        "outputId": "130dc631-14cd-4307-ff32-325908531c25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Scanning gloss folders and checking .npy files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 433.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Num valid samples total: 2065\n",
            "Num missing/invalid files skipped: 0\n",
            "Num classes (after filtering single-sample): 256\n",
            "Num samples after filtering: 2064\n",
            "Num train samples: 1754\n",
            "Num val samples  : 310\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch keypoints shape: torch.Size([32, 64, 150])\n",
            "Batch labels shape   : torch.Size([32])\n",
            "CNNMambaSignModel(\n",
            "  (input_proj): Linear(in_features=150, out_features=256, bias=True)\n",
            "  (cnn_layers): ModuleList(\n",
            "    (0-1): 2 x Sequential(\n",
            "      (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): SiLU()\n",
            "      (3): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (mamba_layers): ModuleList(\n",
            "    (0-1): 2 x MambaBlock(\n",
            "      (in_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
            "      (conv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512)\n",
            "      (out_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "      (act): SiLU()\n",
            "      (dropout): Dropout(p=0.2, inplace=False)\n",
            "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (attn_pool): AttentionPool()\n",
            "  (fc): Sequential(\n",
            "    (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (1): Dropout(p=0.2, inplace=False)\n",
            "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (3): SiLU()\n",
            "    (4): Dropout(p=0.2, inplace=False)\n",
            "    (5): Linear(in_features=256, out_features=256, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Epoch 1/60\n",
            "Train loss: 5.4460 | Top1: 1.16% | Top5: 5.79%\n",
            "Val   loss: 5.1594 | Top1: 2.26% | Top5: 10.32%\n",
            "‚úÖ New best model saved with val_top1=2.26% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 2/60\n",
            "Train loss: 5.0931 | Top1: 2.31% | Top5: 9.72%\n",
            "Val   loss: 4.9876 | Top1: 3.87% | Top5: 13.87%\n",
            "‚úÖ New best model saved with val_top1=3.87% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 3/60\n",
            "Train loss: 4.9180 | Top1: 4.34% | Top5: 12.96%\n",
            "Val   loss: 4.9209 | Top1: 4.19% | Top5: 12.90%\n",
            "‚úÖ New best model saved with val_top1=4.19% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 4/60\n",
            "Train loss: 4.8071 | Top1: 4.75% | Top5: 15.51%\n",
            "Val   loss: 4.9400 | Top1: 4.52% | Top5: 12.58%\n",
            "‚úÖ New best model saved with val_top1=4.52% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 5/60\n",
            "Train loss: 4.6410 | Top1: 6.13% | Top5: 19.79%\n",
            "Val   loss: 4.6644 | Top1: 4.52% | Top5: 20.32%\n",
            "‚úÖ New best model saved with val_top1=4.52% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 6/60\n",
            "Train loss: 4.5306 | Top1: 6.66% | Top5: 22.92%\n",
            "Val   loss: 4.7205 | Top1: 6.77% | Top5: 22.26%\n",
            "‚úÖ New best model saved with val_top1=6.77% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 7/60\n",
            "Train loss: 4.3777 | Top1: 9.03% | Top5: 27.26%\n",
            "Val   loss: 4.7083 | Top1: 6.13% | Top5: 19.35%\n",
            "No improvement. Best val_top1 so far: 6.77%\n",
            "\n",
            "Epoch 8/60\n",
            "Train loss: 4.2830 | Top1: 8.80% | Top5: 29.51%\n",
            "Val   loss: 4.6753 | Top1: 8.06% | Top5: 23.87%\n",
            "‚úÖ New best model saved with val_top1=8.06% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 9/60\n",
            "Train loss: 4.1518 | Top1: 10.71% | Top5: 33.10%\n",
            "Val   loss: 4.5877 | Top1: 8.06% | Top5: 24.52%\n",
            "No improvement. Best val_top1 so far: 8.06%\n",
            "\n",
            "Epoch 10/60\n",
            "Train loss: 4.0214 | Top1: 13.19% | Top5: 37.33%\n",
            "Val   loss: 4.4275 | Top1: 8.71% | Top5: 24.52%\n",
            "‚úÖ New best model saved with val_top1=8.71% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 11/60\n",
            "Train loss: 3.9258 | Top1: 14.18% | Top5: 40.39%\n",
            "Val   loss: 4.6759 | Top1: 9.68% | Top5: 28.71%\n",
            "‚úÖ New best model saved with val_top1=9.68% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 12/60\n",
            "Train loss: 3.8070 | Top1: 16.55% | Top5: 45.08%\n",
            "Val   loss: 4.6270 | Top1: 7.74% | Top5: 26.77%\n",
            "No improvement. Best val_top1 so far: 9.68%\n",
            "\n",
            "Epoch 13/60\n",
            "Train loss: 3.6857 | Top1: 18.29% | Top5: 48.78%\n",
            "Val   loss: 4.5251 | Top1: 10.32% | Top5: 31.61%\n",
            "‚úÖ New best model saved with val_top1=10.32% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 14/60\n",
            "Train loss: 3.5757 | Top1: 18.52% | Top5: 52.72%\n",
            "Val   loss: 4.4725 | Top1: 10.00% | Top5: 30.32%\n",
            "No improvement. Best val_top1 so far: 10.32%\n",
            "\n",
            "Epoch 15/60\n",
            "Train loss: 3.4368 | Top1: 23.38% | Top5: 58.56%\n",
            "Val   loss: 4.6113 | Top1: 10.65% | Top5: 30.00%\n",
            "‚úÖ New best model saved with val_top1=10.65% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 16/60\n",
            "Train loss: 3.3733 | Top1: 25.00% | Top5: 60.42%\n",
            "Val   loss: 4.6131 | Top1: 11.61% | Top5: 32.26%\n",
            "‚úÖ New best model saved with val_top1=11.61% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 17/60\n",
            "Train loss: 3.2183 | Top1: 27.66% | Top5: 65.86%\n",
            "Val   loss: 4.3250 | Top1: 16.13% | Top5: 39.03%\n",
            "‚úÖ New best model saved with val_top1=16.13% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 18/60\n",
            "Train loss: 3.1194 | Top1: 30.73% | Top5: 68.69%\n",
            "Val   loss: 4.1522 | Top1: 17.10% | Top5: 44.19%\n",
            "‚úÖ New best model saved with val_top1=17.10% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 19/60\n",
            "Train loss: 3.0275 | Top1: 34.61% | Top5: 71.35%\n",
            "Val   loss: 4.2570 | Top1: 16.13% | Top5: 43.23%\n",
            "No improvement. Best val_top1 so far: 17.10%\n",
            "\n",
            "Epoch 20/60\n",
            "Train loss: 2.8884 | Top1: 37.62% | Top5: 76.10%\n",
            "Val   loss: 4.6727 | Top1: 12.90% | Top5: 36.77%\n",
            "No improvement. Best val_top1 so far: 17.10%\n",
            "\n",
            "Epoch 21/60\n",
            "Train loss: 2.7806 | Top1: 40.39% | Top5: 79.63%\n",
            "Val   loss: 4.1333 | Top1: 17.74% | Top5: 44.84%\n",
            "‚úÖ New best model saved with val_top1=17.74% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 22/60\n",
            "Train loss: 2.7443 | Top1: 41.84% | Top5: 81.54%\n",
            "Val   loss: 3.9265 | Top1: 22.90% | Top5: 52.90%\n",
            "‚úÖ New best model saved with val_top1=22.90% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 23/60\n",
            "Train loss: 2.5783 | Top1: 46.35% | Top5: 86.23%\n",
            "Val   loss: 4.3089 | Top1: 18.06% | Top5: 45.48%\n",
            "No improvement. Best val_top1 so far: 22.90%\n",
            "\n",
            "Epoch 24/60\n",
            "Train loss: 2.5411 | Top1: 48.84% | Top5: 86.34%\n",
            "Val   loss: 4.1989 | Top1: 17.74% | Top5: 50.65%\n",
            "No improvement. Best val_top1 so far: 22.90%\n",
            "\n",
            "Epoch 25/60\n",
            "Train loss: 2.4332 | Top1: 51.97% | Top5: 88.14%\n",
            "Val   loss: 4.1699 | Top1: 18.71% | Top5: 50.00%\n",
            "No improvement. Best val_top1 so far: 22.90%\n",
            "\n",
            "Epoch 26/60\n",
            "Train loss: 2.3546 | Top1: 53.47% | Top5: 90.80%\n",
            "Val   loss: 4.1340 | Top1: 19.03% | Top5: 50.65%\n",
            "No improvement. Best val_top1 so far: 22.90%\n",
            "\n",
            "Epoch 27/60\n",
            "Train loss: 2.2623 | Top1: 59.14% | Top5: 91.61%\n",
            "Val   loss: 4.1796 | Top1: 21.29% | Top5: 53.55%\n",
            "No improvement. Best val_top1 so far: 22.90%\n",
            "\n",
            "Epoch 28/60\n",
            "Train loss: 2.2199 | Top1: 58.85% | Top5: 92.53%\n",
            "Val   loss: 3.9820 | Top1: 23.55% | Top5: 55.81%\n",
            "‚úÖ New best model saved with val_top1=23.55% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 29/60\n",
            "Train loss: 2.0920 | Top1: 65.51% | Top5: 93.98%\n",
            "Val   loss: 4.4543 | Top1: 22.90% | Top5: 41.94%\n",
            "No improvement. Best val_top1 so far: 23.55%\n",
            "\n",
            "Epoch 30/60\n",
            "Train loss: 2.0817 | Top1: 66.09% | Top5: 94.33%\n",
            "Val   loss: 4.2366 | Top1: 20.97% | Top5: 50.32%\n",
            "No improvement. Best val_top1 so far: 23.55%\n",
            "\n",
            "Epoch 31/60\n",
            "Train loss: 2.0253 | Top1: 68.92% | Top5: 95.14%\n",
            "Val   loss: 4.1198 | Top1: 21.61% | Top5: 54.52%\n",
            "No improvement. Best val_top1 so far: 23.55%\n",
            "\n",
            "Epoch 32/60\n",
            "Train loss: 1.9748 | Top1: 70.37% | Top5: 95.83%\n",
            "Val   loss: 4.2560 | Top1: 21.29% | Top5: 50.65%\n",
            "No improvement. Best val_top1 so far: 23.55%\n",
            "\n",
            "Epoch 33/60\n",
            "Train loss: 1.9163 | Top1: 71.99% | Top5: 96.47%\n",
            "Val   loss: 3.9257 | Top1: 28.71% | Top5: 60.00%\n",
            "‚úÖ New best model saved with val_top1=28.71% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 34/60\n",
            "Train loss: 1.8499 | Top1: 75.58% | Top5: 96.47%\n",
            "Val   loss: 4.0191 | Top1: 26.77% | Top5: 57.10%\n",
            "No improvement. Best val_top1 so far: 28.71%\n",
            "\n",
            "Epoch 35/60\n",
            "Train loss: 1.8246 | Top1: 75.41% | Top5: 97.57%\n",
            "Val   loss: 4.0498 | Top1: 26.45% | Top5: 58.06%\n",
            "No improvement. Best val_top1 so far: 28.71%\n",
            "\n",
            "Epoch 36/60\n",
            "Train loss: 1.8071 | Top1: 77.26% | Top5: 96.88%\n",
            "Val   loss: 4.0047 | Top1: 26.77% | Top5: 58.71%\n",
            "No improvement. Best val_top1 so far: 28.71%\n",
            "\n",
            "Epoch 37/60\n",
            "Train loss: 1.7481 | Top1: 80.03% | Top5: 97.69%\n",
            "Val   loss: 4.0254 | Top1: 29.35% | Top5: 55.81%\n",
            "‚úÖ New best model saved with val_top1=29.35% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 38/60\n",
            "Train loss: 1.6998 | Top1: 82.12% | Top5: 97.63%\n",
            "Val   loss: 4.0820 | Top1: 26.13% | Top5: 58.71%\n",
            "No improvement. Best val_top1 so far: 29.35%\n",
            "\n",
            "Epoch 39/60\n",
            "Train loss: 1.6846 | Top1: 81.89% | Top5: 98.09%\n",
            "Val   loss: 3.9603 | Top1: 29.68% | Top5: 59.03%\n",
            "‚úÖ New best model saved with val_top1=29.68% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 40/60\n",
            "Train loss: 1.6625 | Top1: 82.47% | Top5: 97.97%\n",
            "Val   loss: 3.9230 | Top1: 28.71% | Top5: 58.39%\n",
            "No improvement. Best val_top1 so far: 29.68%\n",
            "\n",
            "Epoch 41/60\n",
            "Train loss: 1.6215 | Top1: 84.66% | Top5: 98.21%\n",
            "Val   loss: 3.9087 | Top1: 30.97% | Top5: 65.16%\n",
            "‚úÖ New best model saved with val_top1=30.97% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 42/60\n",
            "Train loss: 1.6137 | Top1: 84.43% | Top5: 98.50%\n",
            "Val   loss: 4.1032 | Top1: 27.74% | Top5: 56.13%\n",
            "No improvement. Best val_top1 so far: 30.97%\n",
            "\n",
            "Epoch 43/60\n",
            "Train loss: 1.6092 | Top1: 84.72% | Top5: 97.74%\n",
            "Val   loss: 3.8694 | Top1: 30.00% | Top5: 61.94%\n",
            "No improvement. Best val_top1 so far: 30.97%\n",
            "\n",
            "Epoch 44/60\n",
            "Train loss: 1.5660 | Top1: 86.81% | Top5: 98.67%\n",
            "Val   loss: 3.9009 | Top1: 30.97% | Top5: 60.65%\n",
            "No improvement. Best val_top1 so far: 30.97%\n",
            "\n",
            "Epoch 45/60\n",
            "Train loss: 1.5684 | Top1: 86.98% | Top5: 98.21%\n",
            "Val   loss: 3.8687 | Top1: 30.00% | Top5: 61.94%\n",
            "No improvement. Best val_top1 so far: 30.97%\n",
            "\n",
            "Epoch 46/60\n",
            "Train loss: 1.5372 | Top1: 88.08% | Top5: 98.32%\n",
            "Val   loss: 3.8781 | Top1: 31.61% | Top5: 63.23%\n",
            "‚úÖ New best model saved with val_top1=31.61% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 47/60\n",
            "Train loss: 1.5211 | Top1: 87.73% | Top5: 98.44%\n",
            "Val   loss: 3.9065 | Top1: 31.29% | Top5: 61.61%\n",
            "No improvement. Best val_top1 so far: 31.61%\n",
            "\n",
            "Epoch 48/60\n",
            "Train loss: 1.5097 | Top1: 88.48% | Top5: 98.44%\n",
            "Val   loss: 3.8566 | Top1: 30.65% | Top5: 61.29%\n",
            "No improvement. Best val_top1 so far: 31.61%\n",
            "\n",
            "Epoch 49/60\n",
            "Train loss: 1.4996 | Top1: 89.64% | Top5: 98.61%\n",
            "Val   loss: 3.8872 | Top1: 30.97% | Top5: 61.61%\n",
            "No improvement. Best val_top1 so far: 31.61%\n",
            "\n",
            "Epoch 50/60\n",
            "Train loss: 1.4940 | Top1: 90.05% | Top5: 98.55%\n",
            "Val   loss: 3.8994 | Top1: 32.58% | Top5: 60.97%\n",
            "‚úÖ New best model saved with val_top1=32.58% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 51/60\n",
            "Train loss: 1.4749 | Top1: 90.45% | Top5: 98.61%\n",
            "Val   loss: 3.8773 | Top1: 32.26% | Top5: 60.97%\n",
            "No improvement. Best val_top1 so far: 32.58%\n",
            "\n",
            "Epoch 52/60\n",
            "Train loss: 1.4666 | Top1: 90.91% | Top5: 98.44%\n",
            "Val   loss: 3.8805 | Top1: 32.90% | Top5: 60.65%\n",
            "‚úÖ New best model saved with val_top1=32.90% ‚Üí /content/drive/MyDrive/AFML/Dataset/cnn_mamba_wlasl_highacc.pt\n",
            "\n",
            "Epoch 53/60\n",
            "Train loss: 1.4723 | Top1: 90.39% | Top5: 98.50%\n",
            "Val   loss: 3.8741 | Top1: 31.61% | Top5: 61.29%\n",
            "No improvement. Best val_top1 so far: 32.90%\n",
            "\n",
            "Epoch 54/60\n",
            "Train loss: 1.4584 | Top1: 90.34% | Top5: 98.67%\n",
            "Val   loss: 3.8573 | Top1: 31.94% | Top5: 62.26%\n",
            "No improvement. Best val_top1 so far: 32.90%\n",
            "\n",
            "Epoch 55/60\n",
            "Train loss: 1.4653 | Top1: 91.09% | Top5: 98.55%\n",
            "Val   loss: 3.8631 | Top1: 32.26% | Top5: 61.29%\n",
            "No improvement. Best val_top1 so far: 32.90%\n",
            "\n",
            "Epoch 56/60\n",
            "Train loss: 1.4461 | Top1: 91.20% | Top5: 98.73%\n",
            "Val   loss: 3.8614 | Top1: 32.90% | Top5: 61.29%\n",
            "No improvement. Best val_top1 so far: 32.90%\n",
            "\n",
            "Epoch 57/60\n",
            "Train loss: 1.4611 | Top1: 91.26% | Top5: 98.32%\n",
            "Val   loss: 3.8611 | Top1: 32.26% | Top5: 61.94%\n",
            "No improvement. Best val_top1 so far: 32.90%\n",
            "\n",
            "Epoch 58/60\n",
            "Train loss: 1.4565 | Top1: 91.26% | Top5: 98.67%\n",
            "Val   loss: 3.8587 | Top1: 32.26% | Top5: 60.97%\n",
            "No improvement. Best val_top1 so far: 32.90%\n",
            "\n",
            "Epoch 59/60\n",
            "Train loss: 1.4451 | Top1: 91.09% | Top5: 98.78%\n",
            "Val   loss: 3.8605 | Top1: 32.90% | Top5: 61.61%\n",
            "No improvement. Best val_top1 so far: 32.90%\n",
            "\n",
            "Epoch 60/60\n",
            "Train loss: 1.4515 | Top1: 91.72% | Top5: 98.55%\n",
            "Val   loss: 3.8641 | Top1: 32.58% | Top5: 60.32%\n",
            "No improvement. Best val_top1 so far: 32.90%\n",
            "\n",
            "Training finished.\n",
            "Best validation Top-1 accuracy: 32.90%\n"
          ]
        }
      ]
    }
  ]
}